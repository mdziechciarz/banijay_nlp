{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3860487-659f-4ba5-95ec-64072410d2a8",
   "metadata": {},
   "source": [
    "## Importing important libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43e804cb-3cdf-4af6-a80e-c58fcef6918c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d60ff81-1bf9-4d08-9789-1dfa121c3268",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-30 07:11:57.423910: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-30 07:11:57.423958: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-30 07:11:57.424994: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-30 07:11:57.430816: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from transformers import RobertaTokenizer\n",
    "import numpy as np\n",
    "from transformers import TFRobertaForSequenceClassification, RobertaConfig\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import os\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531bfce8-f56d-42a6-830f-27adff87b0e6",
   "metadata": {},
   "source": [
    "## Hyperparameters \n",
    "\n",
    "Easy and straightforward hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75c067a8-3c55-4677-9bae-9fe0b9738f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 1e-5\n",
    "EPOCHS = 30\n",
    "TEST_SIZE = 0.2\n",
    "RANDOM_STATE = 42\n",
    "MAX_LEN = 128  # Maximum sequence length\n",
    "NUM_LABELS = 6\n",
    "\n",
    "# Define the base path for saving model information\n",
    "base_save_path = \"Roberta_V3_1_improve_checkpoints\"\n",
    "checkpoint_path = \"Roberta_V3_1_improve_checkpoints/cp-{epoch:04d}.ckpt\"\n",
    "\n",
    "# Specify the path to your CSV file\n",
    "file_path = 'data/combined_with_synonyms.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbd45f7-81a7-4ef3-b96a-c5aaff50ed63",
   "metadata": {},
   "source": [
    "## F1 Metric and tokenizer function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5739e736-f088-43ec-8357-3a1f569235d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_metric(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856fb3dc-2580-4382-b82a-59e7face147a",
   "metadata": {},
   "source": [
    "## Setup and Data Preparation\n",
    "\n",
    "Loading the dataset, preprocessing the text, and preparing the data for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51e51cca-2af8-4a2f-877b-2924f5fd26cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81930796-7896-4c84-b148-39c70a03cce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna(subset=['emotion']).query(\"emotion != 'neutral'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4cb9a4a8-22fd-4d29-894f-e3cf5d8960ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b36697ce73c34b76b1a93d8fd3d08fda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7475071 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize lemmatizer and stemmer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "tqdm.pandas()\n",
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation and numbers\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = text.split() \n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    # Lemmatize or Stem\n",
    "    words = [lemmatizer.lemmatize(w) for w in words] # Can switch to stemmer.stem(w) if preferred\n",
    "    # Re-join words into a single string\n",
    "    text = ' '.join(words)\n",
    "    return text\n",
    "\n",
    "# Now apply it to your text data column\n",
    "data['preprocessed_sentence'] = data['sentence'].progress_apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be9f4bee-6460-4992-91e0-ff0ef226a2de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preview of the dataset:\n",
      "                                            sentence    emotion  \\\n",
      "0    i feel the tender bruise most whenever ane walk  happiness   \n",
      "1  i could feel moistness starting to gather betw...  happiness   \n",
      "2  One woman , who already had a job and a busy l...    sadness   \n",
      "3  That's not a problem . I'll pop in later today...  happiness   \n",
      "4  i think that thierr particular method is so bu...  happiness   \n",
      "\n",
      "                               preprocessed_sentence  \n",
      "0               feel tender bruise whenever ane walk  \n",
      "1  could feel moistness starting gather leg getti...  \n",
      "2  one woman already job busy life surprised find...  \n",
      "3  thats problem ill pop later today make angstro...  \n",
      "4  think thierr particular method busy gratifing ...  \n",
      "\n",
      "Summary statistics of the  dataset:\n",
      "                           sentence    emotion preprocessed_sentence\n",
      "count                       6591212    6591212               6591212\n",
      "unique                      6591099          6               6022228\n",
      "top     i feel so aroused right now  happiness                      \n",
      "freq                              2    2607913                   275\n",
      "\n",
      "Information about columns in the dataset:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6591212 entries, 0 to 6591211\n",
      "Data columns (total 3 columns):\n",
      " #   Column                 Dtype \n",
      "---  ------                 ----- \n",
      " 0   sentence               object\n",
      " 1   emotion                object\n",
      " 2   preprocessed_sentence  object\n",
      "dtypes: object(3)\n",
      "memory usage: 150.9+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "data = data.drop_duplicates()\n",
    "\n",
    "# Shuffle the combined dataset\n",
    "data = data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Display basic information about the combined dataset\n",
    "print(\"Preview of the dataset:\")\n",
    "print(data.head())\n",
    "\n",
    "print(\"\\nSummary statistics of the  dataset:\")\n",
    "print(data.describe())\n",
    "\n",
    "print(\"\\nInformation about columns in the dataset:\")\n",
    "print(data.info())\n",
    "\n",
    "sentences = data['preprocessed_sentence'].values\n",
    "labels = data['emotion'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eefdd250-2fcc-48bb-8533-9d281741c7bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-30 07:30:45.304227: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2024-03-30 07:30:45.304529: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 19849 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:25:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "# Encode the labels\n",
    "label_encoder = LabelEncoder()\n",
    "labels = label_encoder.fit_transform(labels)\n",
    "labels = tf.convert_to_tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a42bc8f3-0fe3-4751-a063-3d5f61aebaac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ac616024b9c430bb7c159bdf5de27b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing:   0%|          | 0/6591212 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "# Tokenize sentences to get input IDs and attention masks\n",
    "max_len = 128  # or choose a different max length\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "# Wrap sentences with tqdm for a progress bar\n",
    "for sent in tqdm(sentences, desc=\"Tokenizing\"):\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sent,\n",
    "                        add_special_tokens=True,\n",
    "                        max_length=max_len,\n",
    "                        truncation=True,\n",
    "                        padding='max_length',\n",
    "                        return_attention_mask=True,\n",
    "                        return_tensors='tf',\n",
    "                   )\n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "# Convert lists to tensors\n",
    "input_ids = tf.concat(input_ids, axis=0)\n",
    "attention_masks = tf.concat(attention_masks, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4fd3e044-cb31-4e4c-8bcb-e9a892d1b859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure all inputs to train_test_split are numpy arrays\n",
    "input_ids_np = input_ids.numpy() if isinstance(input_ids, tf.Tensor) else input_ids\n",
    "attention_masks_np = attention_masks.numpy() if isinstance(attention_masks, tf.Tensor) else attention_masks\n",
    "labels_np = labels.numpy() if isinstance(labels, tf.Tensor) else labels\n",
    "\n",
    "# Now perform the train-test split\n",
    "X_train, X_val, y_train, y_val = train_test_split(input_ids_np, labels_np, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=labels_np)\n",
    "train_mask, val_mask = train_test_split(attention_masks_np, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=labels_np)\n",
    "\n",
    "# Convert numpy arrays back to tensors for TensorFlow compatibility\n",
    "X_train = tf.constant(X_train)\n",
    "X_val = tf.constant(X_val)\n",
    "y_train = tf.constant(y_train)\n",
    "y_val = tf.constant(y_val)\n",
    "train_mask = tf.constant(train_mask)\n",
    "val_mask = tf.constant(val_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e1634f38-7a61-441b-b2f3-744b167270e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TensorFlow datasets for the training and validation sets\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(({\"input_ids\": X_train, \"attention_mask\": train_mask}, y_train)).shuffle(len(X_train)).batch(BATCH_SIZE)\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices(({\"input_ids\": X_val, \"attention_mask\": val_mask}, y_val)).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867f7842-147f-4d19-b4cd-e33146178208",
   "metadata": {},
   "source": [
    "## Model training\n",
    "\n",
    "Setting up the RoBERTa model, defining the training loop, and initiating the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "37201084-a8e1-432b-b635-ed3ae665f5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dropout, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d570dd00-ea60-41e6-8842-260e51f860ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model CustomTFRobertaForSequenceClassification: ['roberta.embeddings.position_ids']\n",
      "- This IS expected if you are initializing CustomTFRobertaForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CustomTFRobertaForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of CustomTFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use CustomTFRobertaForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# Load the configuration from the RoBERTa model\n",
    "model_config = RobertaConfig.from_pretrained('roberta-base', num_labels=NUM_LABELS)\n",
    "\n",
    "# Load the pre-trained RoBERTa model\n",
    "class CustomRobertaClassificationHead(tf.keras.layers.Layer):\n",
    "    \"\"\"Custom classification head for RoBERTa with additional dropout.\"\"\"\n",
    "\n",
    "    def __init__(self, config, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.dense = tf.keras.layers.Dense(\n",
    "            config.hidden_size, activation=\"relu\", name=\"dense\"\n",
    "        )\n",
    "        self.dropout = Dropout(0.5)  # Example dropout rate of 0.5\n",
    "        self.out_proj = tf.keras.layers.Dense(\n",
    "            config.num_labels, activation=None, name=\"out_proj\"\n",
    "        )\n",
    "\n",
    "    def call(self, features, *args, **kwargs):\n",
    "        x = features[:, 0, :]  # take <s> token (equiv. to [CLS])\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.out_proj(x)\n",
    "        return x\n",
    "\n",
    "# Subclass the model to replace the classification head\n",
    "class CustomTFRobertaForSequenceClassification(TFRobertaForSequenceClassification):\n",
    "    def __init__(self, config, *args, **kwargs):\n",
    "        super().__init__(config, *args, **kwargs)\n",
    "        self.classifier = CustomRobertaClassificationHead(config)\n",
    "\n",
    "# Initialize the custom model\n",
    "model = CustomTFRobertaForSequenceClassification.from_pretrained(\n",
    "    'roberta-base',\n",
    "    config=model_config\n",
    ")\n",
    "# Compile the model\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "# Compile the model with your chosen optimizer, loss, and metrics\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy', f1_metric])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5a9193d4-8db2-4b46-8034-0d403b5f994d",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_accuracy',  # Monitor validation accuracy\n",
    "    patience=2,             # Number of epochs with no improvement after which training will be stopped\n",
    "    verbose=1,              # Log when training is stopped\n",
    "    restore_best_weights=True, # Restore model weights from the epoch with the best value of the monitored quantity\n",
    "    mode='max'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9f36a529-e4ed-4115-b45c-b44cc3f9910a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,  # More aggressive reduction\n",
    "    patience=1,  # Reduced patience\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cd642749-da1b-4160-95c2-8db091c894fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit label encoder and return encoded labels\n",
    "labels_encoded = label_encoder.fit_transform(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "59fae809-f665-41e4-9bcf-850d40a11b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of occurrences of each class\n",
    "class_counts = np.unique(labels_encoded, return_counts=True)[1]\n",
    "\n",
    "# Calculate total number of samples\n",
    "total_samples = len(labels_encoded)\n",
    "\n",
    "# Calculate class weights inversely proportional to the class frequencies\n",
    "class_weights = {i: total_samples/(count * len(class_counts)) for i, count in enumerate(class_counts)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a4875774-e62e-4de9-8f48-8de9746d5d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the directory where you want to save the checkpoints\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "# Ensure the directory exists. If it doesn't, create it.\n",
    "os.makedirs(base_save_path, exist_ok=True)\n",
    "\n",
    "# Specify the checkpoint file path pattern\n",
    "checkpoint_path = os.path.join(base_save_path, \"cp-{epoch:04d}.ckpt\")\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "# Create a ModelCheckpoint callback\n",
    "cp_callback = ModelCheckpoint(\n",
    "    filepath=checkpoint_path, \n",
    "    verbose=1, \n",
    "    save_weights_only=True, \n",
    "    save_best_only=True,  # Saves only the best model\n",
    "    monitor='val_loss',  # Monitoring validation loss to determine the best model\n",
    "    mode='min',  # Since we're monitoring 'val_loss', 'min' mode saves the model when the metric has decreased\n",
    "    save_freq='epoch')  # Saving the model after each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77f9bbd-4fda-40d1-b49a-b75e8f6eb9c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-30 08:49:38.523449: I external/local_xla/xla/service/service.cc:168] XLA service 0x7f33848b47f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-03-30 08:49:38.523482: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2024-03-30 08:49:38.528925: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-03-30 08:49:38.559123: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8906\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1711788578.665824  903932 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53191/82391 [==================>...........] - ETA: 7:19:04 - loss: 0.2519 - accuracy: 0.9156 - f1_metric: 1.3595"
     ]
    }
   ],
   "source": [
    "# Fit the model with class weights, early stopping, and learning rate scheduler\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=val_dataset,\n",
    "    callbacks=[early_stopping, lr_scheduler, cp_callback],\n",
    "    class_weight=class_weights  # Use the calculated class weights\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f1b6e5-e6e1-4ddb-bb51-01b727d79713",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.evaluate(val_dataset)\n",
    "print(f\"Validation loss: {results[0]}, Validation accuracy: {results[1]}, Validation F1 Score: {results[2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88aa3bad-ff0e-4cd0-8d2f-97681e658cf7",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "Generate usefull insights on the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cf99fa-62ee-4208-b3d4-415308ac8a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history):\n",
    "    fig, ax = plt.subplots(1, 3, figsize=(20, 5))\n",
    "    \n",
    "    # Plot training & validation accuracy values\n",
    "    ax[0].plot(history.history['accuracy'])\n",
    "    ax[0].plot(history.history['val_accuracy'])\n",
    "    ax[0].set_title('Model accuracy')\n",
    "    ax[0].set_ylabel('Accuracy')\n",
    "    ax[0].set_xlabel('Epoch')\n",
    "    ax[0].legend(['Train', 'Val'], loc='upper left')\n",
    "\n",
    "    # Plot training & validation loss values\n",
    "    ax[1].plot(history.history['loss'])\n",
    "    ax[1].plot(history.history['val_loss'])\n",
    "    ax[1].set_title('Model loss')\n",
    "    ax[1].set_ylabel('Loss')\n",
    "    ax[1].set_xlabel('Epoch')\n",
    "    ax[1].legend(['Train', 'Val'], loc='upper left')\n",
    "    \n",
    "    # Plot training & validation F1 score values\n",
    "    ax[2].plot(history.history['f1_metric'])\n",
    "    ax[2].plot(history.history['val_f1_metric'])\n",
    "    ax[2].set_title('Model F1 Score')\n",
    "    ax[2].set_ylabel('F1 Score')\n",
    "    ax[2].set_xlabel('Epoch')\n",
    "    ax[2].legend(['Train', 'Val'], loc='upper left')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "plot_training_history(history)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d46d917-0eca-42a8-b5bc-5389d7250f70",
   "metadata": {},
   "source": [
    "##  Generate the Confusion Matrix and Metrics\n",
    "\n",
    "With the true labels and predictions, we can now generate a confusion matrix and calculate other evaluation metrics like precision, recall, and F1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68483e9-474e-44ae-9a33-3202ece3125a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming X_val and y_val are your test inputs and labels, respectively.\n",
    "# If you have a separate test set, replace X_val and y_val accordingly.\n",
    "\n",
    "# Predict classes with the model\n",
    "predictions = model.predict({\"input_ids\": X_val, \"attention_mask\": val_mask})\n",
    "predicted_labels = np.argmax(predictions.logits, axis=1)\n",
    "\n",
    "# Since your labels are already encoded with LabelEncoder, there's no need to decode them\n",
    "\n",
    "# Generate the confusion matrix\n",
    "cm = confusion_matrix(y_val, predicted_labels)\n",
    "\n",
    "# Calculate per-class accuracies\n",
    "accuracies = cm.diagonal() / cm.sum(axis=1)\n",
    "\n",
    "# Create an annotated confusion matrix with accuracies\n",
    "cm_with_acc = cm.astype('str')  # Convert counts to string for annotation\n",
    "for i in range(len(cm)):\n",
    "    for j in range(len(cm[i])):\n",
    "        # Append accuracy to the diagonal elements\n",
    "        if i == j:\n",
    "            cm_with_acc[i][j] = f\"{cm[i][j]}\\n({accuracies[i]*100:.1f}%)\"\n",
    "        else:\n",
    "            cm_with_acc[i][j] += \"\\n \"\n",
    "\n",
    "# Plotting\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=cm_with_acc, fmt='', ax=ax, cmap='Blues', \n",
    "            xticklabels=label_encoder.classes_, \n",
    "            yticklabels=label_encoder.classes_)\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.title('Confusion Matrix with Accuracies')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95905cbc-5b2e-4c2e-a361-fde96dabbac5",
   "metadata": {},
   "source": [
    "## Prepare the Submission DataFrame and Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8001b0c2-e8a2-4f15-84a4-8c5fd17a6bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the test dataset\n",
    "test_df = pd.read_csv('test (1).csv', sep='\\t')\n",
    "\n",
    "# Replace the exclamation point at the end of each sentence with a dot\n",
    "test_df['sentence'] = test_df['sentence'].str.replace(r'!\\s*$', '.', regex=True)\n",
    "\n",
    "# Assuming 'sentence' is your column of interest and tokenizer is already defined\n",
    "# Prepare the test sentences with RobertaTokenizer\n",
    "test_input_ids = []\n",
    "test_attention_masks = []\n",
    "\n",
    "for sent in test_df['sentence']:\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sent,                        # Sentence to encode\n",
    "                        add_special_tokens = True,   # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 128,            # Pad & truncate all sentences\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,# Construct attention masks\n",
    "                        return_tensors = 'tf',       # Return tf tensors\n",
    "                   )\n",
    "    \n",
    "    # Add the encoded sentence to the list\n",
    "    test_input_ids.append(encoded_dict['input_ids'])\n",
    "    test_attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "# Convert lists into tensors\n",
    "test_input_ids = tf.concat(test_input_ids, axis=0)\n",
    "test_attention_masks = tf.concat(test_attention_masks, axis=0)\n",
    "\n",
    "# Make predictions using the trained model\n",
    "predictions = model.predict({\"input_ids\": test_input_ids, \"attention_mask\": test_attention_masks})\n",
    "predicted_labels = np.argmax(predictions.logits, axis=1)\n",
    "\n",
    "# Convert numeric predictions back to original labels using the LabelEncoder\n",
    "predicted_emotions = label_encoder.inverse_transform(predicted_labels)\n",
    "\n",
    "# Prepare the submission DataFrame\n",
    "submission_df = pd.DataFrame({\n",
    "    'id': test_df['id'],  # Assuming there's an 'id' column in your test set\n",
    "    'emotion': predicted_emotions\n",
    "})\n",
    "\n",
    "# Save the submission file\n",
    "submission_df.to_csv('Roberta_V3_1_improve.csv', index=False, sep=',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2daffd60-882e-48d9-8501-7d35f341f851",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Define the base path for saving\n",
    "base_save_path = \"Roberta_V3_1_improve\"\n",
    "\n",
    "# Specify the paths for the model, tokenizer, and label encoder\n",
    "model_save_path = f\"{base_save_path}/model\"\n",
    "tokenizer_save_path = f\"{base_save_path}/tokenizer\"\n",
    "label_encoder_save_path = f\"{base_save_path}/label_encoder.joblib\"\n",
    "\n",
    "# Assuming `model` is your TFRobertaForSequenceClassification model,\n",
    "# `tokenizer` is your RobertaTokenizer, and `label_encoder` is your LabelEncoder instance\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained(model_save_path)\n",
    "\n",
    "# Save the tokenizer\n",
    "tokenizer.save_pretrained(tokenizer_save_path)\n",
    "\n",
    "# Save the label encoder\n",
    "joblib.dump(label_encoder, label_encoder_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f5a1c8-fe06-4b18-b0b3-7f8008df9a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sub = 'Roberta_V3_1_improve.csv'\n",
    "\n",
    "# Load the dataset\n",
    "data_submission = pd.read_csv(data_sub)\n",
    "\n",
    "# Define the mapping from integer labels to emotion names\n",
    "emotion_mapping = {\n",
    "    0: 'anger',\n",
    "    1: 'disgust',\n",
    "    2: 'fear',\n",
    "    3: 'happiness',\n",
    "    4: 'sadness',\n",
    "    5: 'surprise'\n",
    "}\n",
    "\n",
    "# Apply the mapping to the 'emotion' column\n",
    "data_submission['emotion'] = data_submission['emotion'].map(emotion_mapping)\n",
    "# Save the submission file\n",
    "data_submission.to_csv('Roberta_V3_1_improve.csv', index=False, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7602b931-c659-4489-82b3-365c26ae6be9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0be143f-f829-40e7-8cb9-066b489fd6cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
