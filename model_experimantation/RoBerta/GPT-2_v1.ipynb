{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3860487-659f-4ba5-95ec-64072410d2a8",
   "metadata": {},
   "source": [
    "## Importing important libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25d87e6f-eb5c-4f2d-8fc7-89c853f32fe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-03 18:47:45.273351: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-04-03 18:47:45.273404: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-04-03 18:47:45.274811: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-04-03 18:47:45.281760: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from transformers import GPT2Tokenizer, TFGPT2Model, GPT2Config\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "import os\n",
    "\n",
    "# Hyperparameters\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 1e-5\n",
    "EPOCHS = 30\n",
    "TEST_SIZE = 0.2\n",
    "RANDOM_STATE = 42\n",
    "MAX_LEN = 128\n",
    "\n",
    "FILE_PATH = 'emotion_data_merged_4.csv'\n",
    "# Define the base path for saving model information\n",
    "base_save_path = \"GPT-2_checkpoints\"\n",
    "checkpoint_path = \"GPT-2_checkpoints/cp-{epoch:04d}.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e89fd9e7-0cc6-4ad2-a9d0-52b3ae467448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dictionary to map encoded emotions to their corresponding labels\n",
    "emotion_labels = {0: 'anger', 1: 'fear', 2: 'joy', 3: 'sadness', 4: 'surprise', 5: 'neutral'}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d60ff81-1bf9-4d08-9789-1dfa121c3268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset = pd.read_csv(FILE_PATH)\n",
    "dataset = dataset.dropna(subset=['emotion']).query(\"emotion != 'neutral'\")\n",
    "dataset = dataset.drop_duplicates()\n",
    "# Assuming you start with 'sentence' and 'emotion' columns\n",
    "\n",
    "# Encode the 'emotion' column to numeric labels\n",
    "label_encoder = LabelEncoder()\n",
    "dataset['encoded_emotion'] = label_encoder.fit_transform(dataset['emotion'])\n",
    "\n",
    "# Create prompts without merging the labels into the prompts\n",
    "dataset['prompt'] = \"This text expresses: \" + dataset['sentence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14125bbd-0687-47e6-8480-61fdaec1cff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset while keeping prompts and labels separate\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    dataset['prompt'], dataset['encoded_emotion'], \n",
    "    test_size=TEST_SIZE, \n",
    "    random_state=RANDOM_STATE, \n",
    "    stratify=dataset['encoded_emotion']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bac98088-884b-4b5d-887b-a86009b70c65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-03 18:47:50.789323: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2024-04-03 18:47:50.789595: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 11131 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "def encode_prompts(prompts):\n",
    "    input_ids, attention_masks = [], []\n",
    "    for prompt in prompts:\n",
    "        encoded = tokenizer(prompt, padding='max_length', truncation=True, max_length=MAX_LEN, return_tensors=\"tf\")\n",
    "        input_ids.append(encoded['input_ids'])\n",
    "        attention_masks.append(encoded['attention_mask'])\n",
    "    return tf.concat(input_ids, axis=0), tf.concat(attention_masks, axis=0)\n",
    "\n",
    "# Encode prompts using tokenizer (shown previously)\n",
    "input_ids_train, attention_masks_train = encode_prompts(X_train.tolist())\n",
    "input_ids_val, attention_masks_val = encode_prompts(X_val.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18e59280-ee5e-45f3-97bc-15c9f2fdb44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit label encoder and return encoded labels\n",
    "labels_encoded = dataset['encoded_emotion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a417bb2f-5ed8-47de-ab1e-3472d1c32954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of occurrences of each class\n",
    "class_counts = np.unique(labels_encoded, return_counts=True)[1]\n",
    "\n",
    "# Calculate total number of samples\n",
    "total_samples = len(labels_encoded)\n",
    "\n",
    "# Calculate class weights inversely proportional to the class frequencies\n",
    "class_weights = {i: total_samples/(count * len(class_counts)) for i, count in enumerate(class_counts)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e4fbcb4-f63e-45bc-8160-ca61720dd413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the directory where you want to save the checkpoints\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "# Ensure the directory exists. If it doesn't, create it.\n",
    "os.makedirs(base_save_path, exist_ok=True)\n",
    "\n",
    "# Specify the checkpoint file path pattern\n",
    "checkpoint_path = os.path.join(base_save_path, \"cp-{epoch:04d}.ckpt\")\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "# Create a ModelCheckpoint callback\n",
    "cp_callback = ModelCheckpoint(\n",
    "    filepath=checkpoint_path, \n",
    "    verbose=1, \n",
    "    save_weights_only=True, \n",
    "    save_best_only=True,  # Saves only the best model\n",
    "    monitor='val_loss',  # Monitoring validation loss to determine the best model\n",
    "    mode='min',  # Since we're monitoring 'val_loss', 'min' mode saves the model when the metric has decreased\n",
    "    save_freq='epoch')  # Saving the model after each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9205425e-82b5-46d0-b2c1-9ff78063dfec",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.3,  # Reduction factor for the learning rate\n",
    "    patience=2,  # Number of epochs with no improvement after which learning rate will be reduced\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ba9ea2d-862d-4280-8247-acb960a023d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_accuracy',  # Monitor validation accuracy\n",
    "    patience=3,             # Number of epochs with no improvement after which training will be stopped\n",
    "    verbose=1,              # Log when training is stopped\n",
    "    restore_best_weights=True, # Restore model weights from the epoch with the best value of the monitored quantity\n",
    "    mode='max'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "67f938a1-2d63-44e8-9da3-9b8598b5394d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFGPT2Model.\n",
      "\n",
      "All the weights of TFGPT2Model were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2Model for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "def build_model(num_labels):\n",
    "    # Load the pre-trained GPT-2 model configuration\n",
    "    config = GPT2Config.from_pretrained('gpt2', num_labels=num_labels)\n",
    "    # Load the GPT-2 model from its configuration\n",
    "    gpt2_model = TFGPT2Model.from_pretrained('gpt2', config=config)\n",
    "    \n",
    "    # Define the inputs\n",
    "    input_ids = tf.keras.Input(shape=(MAX_LEN,), dtype=tf.int32, name=\"input_ids\")\n",
    "    attention_mask = tf.keras.Input(shape=(MAX_LEN,), dtype=tf.int32, name=\"attention_mask\")\n",
    "    \n",
    "    # Get the outputs from the GPT-2 model\n",
    "    outputs = gpt2_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    sequence_output = outputs.last_hidden_state[:, -1, :]  # Use the last hidden state\n",
    "    \n",
    "    # Add a dense layer for classification\n",
    "    classifier_layer = tf.keras.layers.Dense(num_labels, activation='softmax', name='classifier')(sequence_output)\n",
    "    \n",
    "    # Construct the final model\n",
    "    model = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=classifier_layer)\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = build_model(num_labels=6)  # Set num_labels to 6 for multi-class classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "318715a8-b25d-44e8-9dc5-1b5bc5028974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y_train: (383312,)\n",
      "Shape of y_val: (95828,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of y_train:\", y_train.shape)  # Expected to be something like (n_samples,)\n",
    "print(\"Shape of y_val:\", y_val.shape)     # Expected to be something like (n_samples,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af7d66f9-a181-4e47-8504-54ebfe633bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.astype('int32')\n",
    "y_val = y_val.astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "17250d64-e747-4b23-a1bf-a5ad13a61719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y_train: (383312,)\n",
      "Shape of y_val: (95828,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of y_train:\", y_train.shape)  # Expected to be something like (n_samples,)\n",
    "print(\"Shape of y_val:\", y_val.shape)     # Expected to be something like (n_samples,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e0d4782e-29e6-4816-a081-735215fe2e00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)      [(None, 128)]                0         []                            \n",
      "                                                                                                  \n",
      " attention_mask (InputLayer  [(None, 128)]                0         []                            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tfgpt2_model (TFGPT2Model)  TFBaseModelOutputWithPastA   1244398   ['input_ids[0][0]',           \n",
      "                             ndCrossAttentions(last_hid   08         'attention_mask[0][0]']      \n",
      "                             den_state=(None, 128, 768)                                           \n",
      "                             , past_key_values=((2, Non                                           \n",
      "                             e, 12, 128, 64),                                                     \n",
      "                              (2, None, 12, 128, 64),                                             \n",
      "                              (2, None, 12, 128, 64),                                             \n",
      "                              (2, None, 12, 128, 64),                                             \n",
      "                              (2, None, 12, 128, 64),                                             \n",
      "                              (2, None, 12, 128, 64),                                             \n",
      "                              (2, None, 12, 128, 64),                                             \n",
      "                              (2, None, 12, 128, 64),                                             \n",
      "                              (2, None, 12, 128, 64),                                             \n",
      "                              (2, None, 12, 128, 64),                                             \n",
      "                              (2, None, 12, 128, 64),                                             \n",
      "                              (2, None, 12, 128, 64)),                                            \n",
      "                              hidden_states=None, atten                                           \n",
      "                             tions=None, cross_attentio                                           \n",
      "                             ns=None)                                                             \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem (  (None, 768)                  0         ['tfgpt2_model[0][0]']        \n",
      " SlicingOpLambda)                                                                                 \n",
      "                                                                                                  \n",
      " classifier (Dense)          (None, 6)                    4614      ['tf.__operators__.getitem[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 124444422 (474.72 MB)\n",
      "Trainable params: 124444422 (474.72 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Final layer check\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a3784f69-82e5-411b-b482-96124c340e06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs shape: (383312, 128)\n",
      "Attention Masks shape: (383312, 128)\n",
      "Labels shape: (383312,)\n",
      "Labels type: int32\n"
     ]
    }
   ],
   "source": [
    "print(\"Input IDs shape:\", input_ids_train.shape)\n",
    "print(\"Attention Masks shape:\", attention_masks_train.shape)\n",
    "print(\"Labels shape:\", y_train.shape)\n",
    "print(\"Labels type:\", y_train.dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "22134406-c277-42be-a165-d106333159b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique labels in y_train: [0 1 2 3 4 5]\n",
      "Unique labels in y_val: [0 1 2 3 4 5]\n",
      "Number of unique labels: 6\n",
      "Shape of y_train: (383312,)\n",
      "Shape of y_val: (95828,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Unique labels in y_train:\", np.unique(y_train))\n",
    "print(\"Unique labels in y_val:\", np.unique(y_val))\n",
    "print(\"Number of unique labels:\", len(np.unique(np.concatenate([y_train, y_val]))))\n",
    "print(\"Shape of y_train:\", y_train.shape)\n",
    "print(\"Shape of y_val:\", y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "be7be696-e19f-459b-ad76-e467d49e1be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs shape (train): (383312, 128)\n",
      "Attention Masks shape (train): (383312, 128)\n",
      "Input IDs shape (val): (95828, 128)\n",
      "Attention Masks shape (val): (95828, 128)\n"
     ]
    }
   ],
   "source": [
    "print(\"Input IDs shape (train):\", input_ids_train.shape)\n",
    "print(\"Attention Masks shape (train):\", attention_masks_train.shape)\n",
    "print(\"Input IDs shape (val):\", input_ids_val.shape)\n",
    "print(\"Attention Masks shape (val):\", attention_masks_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d0535e11-240c-46ad-b41d-4c2297d64ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few labels in y_train: 72541     4\n",
      "108553    3\n",
      "659867    1\n",
      "288572    0\n",
      "525872    1\n",
      "233682    5\n",
      "236228    3\n",
      "247269    0\n",
      "47555     3\n",
      "370301    0\n",
      "Name: encoded_emotion, dtype: int32\n",
      "First few labels in y_val: 399915    3\n",
      "423120    3\n",
      "49249     3\n",
      "187865    3\n",
      "15332     2\n",
      "91596     3\n",
      "73119     3\n",
      "350455    3\n",
      "524762    2\n",
      "411584    0\n",
      "Name: encoded_emotion, dtype: int32\n"
     ]
    }
   ],
   "source": [
    "print(\"First few labels in y_train:\", y_train[:10])\n",
    "print(\"First few labels in y_val:\", y_val[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c5f1a3fd-966a-4b6e-a49b-3f7c309699ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few labels in y_train:\n",
      "Label: 4 | Emotion: surprise\n",
      "Label: 3 | Emotion: sadness\n",
      "Label: 1 | Emotion: fear\n",
      "Label: 0 | Emotion: anger\n",
      "Label: 1 | Emotion: fear\n",
      "Label: 5 | Emotion: neutral\n",
      "Label: 3 | Emotion: sadness\n",
      "Label: 0 | Emotion: anger\n",
      "Label: 3 | Emotion: sadness\n",
      "Label: 0 | Emotion: anger\n",
      "\n",
      "First few labels in y_val:\n",
      "Label: 3 | Emotion: sadness\n",
      "Label: 3 | Emotion: sadness\n",
      "Label: 3 | Emotion: sadness\n",
      "Label: 3 | Emotion: sadness\n",
      "Label: 2 | Emotion: joy\n",
      "Label: 3 | Emotion: sadness\n",
      "Label: 3 | Emotion: sadness\n",
      "Label: 3 | Emotion: sadness\n",
      "Label: 2 | Emotion: joy\n",
      "Label: 0 | Emotion: anger\n"
     ]
    }
   ],
   "source": [
    "# Print the first few labels and their corresponding emotions for y_train\n",
    "print(\"First few labels in y_train:\")\n",
    "for index, label in y_train[:10].items():\n",
    "    print(\"Label:\", label, \"| Emotion:\", emotion_labels[label])\n",
    "\n",
    "# Print the first few labels and their corresponding emotions for y_val\n",
    "print(\"\\nFirst few labels in y_val:\")\n",
    "for index, label in y_val[:10].items():\n",
    "    print(\"Label:\", label, \"| Emotion:\", emotion_labels[label])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4bb5d871-2fd0-4bfd-96dc-35fb83d53966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "11974/11979 [============================>.] - ETA: 0s"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unexpected result of `train_function` (Empty logs). This could be due to issues in input pipeline that resulted in an empty dataset. Otherwise, please use `Model.compile(..., run_eagerly=True)`, or `tf.config.run_functions_eagerly(True)` for more information of where went wrong, or file a issue/bug to `tf.keras`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Proceed with model training as previously outlined, ensuring class weights and callbacks are applied\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43minput_ids_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_masks_train\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43minput_ids_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_masks_val\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Ensure class weights are correctly applied\u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr_scheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcp_callback\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/engine/training.py:1819\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1817\u001b[0m logs \u001b[38;5;241m=\u001b[39m tf_utils\u001b[38;5;241m.\u001b[39msync_to_numpy_or_python_type(logs)\n\u001b[1;32m   1818\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m logs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1819\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1820\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected result of `train_function` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1821\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(Empty logs). This could be due to issues in input \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1822\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpipeline that resulted in an empty dataset. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1823\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOtherwise, please use \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1824\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`Model.compile(..., run_eagerly=True)`, or \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1825\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`tf.config.run_functions_eagerly(True)` for more \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1826\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minformation of where went wrong, or file a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1827\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124missue/bug to `tf.keras`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1828\u001b[0m     )\n\u001b[1;32m   1829\u001b[0m \u001b[38;5;66;03m# Override with model metrics instead of last step logs\u001b[39;00m\n\u001b[1;32m   1830\u001b[0m logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_and_get_metrics_result(logs)\n",
      "\u001b[0;31mValueError\u001b[0m: Unexpected result of `train_function` (Empty logs). This could be due to issues in input pipeline that resulted in an empty dataset. Otherwise, please use `Model.compile(..., run_eagerly=True)`, or `tf.config.run_functions_eagerly(True)` for more information of where went wrong, or file a issue/bug to `tf.keras`."
     ]
    }
   ],
   "source": [
    "# Proceed with model training as previously outlined, ensuring class weights and callbacks are applied\n",
    "history = model.fit(\n",
    "    [input_ids_train, attention_masks_train], y_train,\n",
    "    validation_data=([input_ids_val, attention_masks_val], y_val),\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_weight=class_weights,  # Ensure class weights are correctly applied\n",
    "    callbacks=[early_stopping, lr_scheduler, cp_callback],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794ed1c6-c90c-4a49-b7e3-b4029d72cf5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "labels = dataset['encoded_emotion'].values\n",
    "X_train, X_val, y_train, y_val = train_test_split(input_ids, labels, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=labels)\n",
    "\n",
    "# Convert to TensorFlow datasets for efficiency\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(({\"input_ids\": X_train, \"attention_mask\": attention_masks[X_train.index]}), y_train).shuffle(10000).batch(BATCH_SIZE)\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices(({\"input_ids\": X_val, \"attention_mask\": attention_masks[X_val.index]}), y_val).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6d0323-9479-4f6c-961c-5bc413a39ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate class weights\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_train),\n",
    "    y=y_train\n",
    ")\n",
    "\n",
    "class_weights_dict = {i: weight for i, weight in enumerate(class_weights)}\n",
    "print(\"Class weights:\", class_weights_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61c88c9-827d-47d9-8f51-686cbb200940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "os.makedirs(BASE_SAVE_PATH, exist_ok=True)\n",
    "checkpoint_path = os.path.join(BASE_SAVE_PATH, \"cp-{epoch:04d}.ckpt\")\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=3, verbose=1, restore_best_weights=True),\n",
    "    ModelCheckpoint(filepath=checkpoint_path, monitor='val_loss', save_best_only=True, save_weights_only=True, verbose=1),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=2, verbose=1)\n",
    "]\n",
    "\n",
    "# Training\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=callbacks,\n",
    "    class_weight=class_weights_dict\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7d654e-fb0c-44cc-a4cd-69342e043d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "val_loss, val_accuracy = model.evaluate(val_dataset)\n",
    "print(f\"Validation loss: {val_loss}, Validation accuracy: {val_accuracy}\")\n",
    "\n",
    "# Inference example\n",
    "def predict_emotion(text):\n",
    "    prompt = \"This text expresses: \" + text + \" The emotion is: [LABEL]\"\n",
    "    encoded_prompt = tokenizer.encode_plus(\n",
    "        prompt, add_special_tokens=True, max_length=MAX_LEN, padding='max_length',\n",
    "        truncation=True, return_attention_mask=True, return_tensors='tf'\n",
    "    )\n",
    "    input_ids = encoded_prompt['input_ids']\n",
    "    attention_mask = encoded_prompt['attention_mask']\n",
    "    \n",
    "    predictions = model.predict({'input_ids': input_ids, 'attention_mask': attention_mask})\n",
    "    predicted_label_idx = tf.argmax(predictions, axis=1).numpy()[0]\n",
    "    predicted_label = label_encoder.inverse_transform([predicted_label_idx])[0]\n",
    "    return predicted_label\n",
    "\n",
    "# Example inference\n",
    "sample_text = \"I feel so happy and joyful today!\"\n",
    "predicted_emotion = predict_emotion(sample_text)\n",
    "print(f\"The predicted emotion for '{sample_text}' is: {predicted_emotion}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531bfce8-f56d-42a6-830f-27adff87b0e6",
   "metadata": {},
   "source": [
    "## Hyperparameters \n",
    "\n",
    "Easy and straightforward hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75c067a8-3c55-4677-9bae-9fe0b9738f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 1e-5\n",
    "EPOCHS = 30\n",
    "TEST_SIZE = 0.2\n",
    "RANDOM_STATE = 42\n",
    "MAX_LEN = 128  # Maximum sequence length\n",
    "NUM_LABELS = None  # This will be set after loading the data\n",
    "\n",
    "# Paths\n",
    "FILE_PATH = 'Tasks/emotion_data_merged_4.csv'\n",
    "BASE_SAVE_PATH = \"GPT-2_v1_checkpoints\"\n",
    "CHECKPOINT_PATH = \"GPT-2_v1_checkpoints/cp-{epoch:04d}.ckpt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbd45f7-81a7-4ef3-b96a-c5aaff50ed63",
   "metadata": {},
   "source": [
    "## F1 Metric and tokenizer function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5739e736-f088-43ec-8357-3a1f569235d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_metric(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5eb25d61-feab-48da-99fd-7edb9f73b23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_label_from_generated_text(generated_text):\n",
    "    # Assuming labels is a list of label names\n",
    "    predicted_label_idx = tf.argmax(generated_text, axis=-1)\n",
    "    with tf.compat.v1.Session() as sess:  # Using a TensorFlow session to evaluate the tensor\n",
    "        predicted_label_idx_value = sess.run(predicted_label_idx)\n",
    "    predicted_label = labels[predicted_label_idx_value]\n",
    "    return predicted_label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9268fd2-992b-4831-9930-be5631210058",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExactMatchAccuracy(tf.keras.metrics.Metric):\n",
    "    def __init__(self, name='exact_match_accuracy', **kwargs):\n",
    "        super(ExactMatchAccuracy, self).__init__(name=name, **kwargs)\n",
    "        self.correct_predictions = self.add_weight(name='cp', initializer='zeros')\n",
    "        self.total_predictions = self.add_weight(name='tp', initializer='zeros')\n",
    "    \n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        with ag__.FunctionScope('update_state', 'fscope', ag__.STD) as fscope:\n",
    "            y_pred_texts = [extract_label_from_generated_text(text) for text in tf.unstack(y_pred, axis=-1)]\n",
    "            y_true_texts = [tf.strings.decode(label) for label in tf.unstack(y_true, axis=-1)]\n",
    "        \n",
    "        for true, pred in zip(y_true_texts, y_pred_texts):\n",
    "            if true == pred:\n",
    "                self.correct_predictions.assign_add(1)\n",
    "            self.total_predictions.assign_add(1)\n",
    "\n",
    "    def result(self):\n",
    "        return self.correct_predictions / self.total_predictions\n",
    "\n",
    "    def reset_states(self):\n",
    "        self.correct_predictions.assign(0)\n",
    "        self.total_predictions.assign(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856fb3dc-2580-4382-b82a-59e7face147a",
   "metadata": {},
   "source": [
    "## Setup and Data Preparation\n",
    "\n",
    "Loading the dataset, preprocessing the text, and preparing the data for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e22f5db7-7207-422e-9c13-c5a4dd39415e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preview of the dataset:\n",
      "                                            sentence    emotion  \\\n",
      "0  im feeling its time to go their presence is no...    sadness   \n",
      "1  i handle it this time around should i be very ...  happiness   \n",
      "2  i feel more and more like a murderer of innoce...  happiness   \n",
      "3  i really shouldnt rant when im feeling like th...  happiness   \n",
      "4  i do not ever recall feeling the searing inten...  happiness   \n",
      "\n",
      "                                              prompt  \n",
      "0  This text expresses: im feeling its time to go...  \n",
      "1  This text expresses: i handle it this time aro...  \n",
      "2  This text expresses: i feel more and more like...  \n",
      "3  This text expresses: i really shouldnt rant wh...  \n",
      "4  This text expresses: i do not ever recall feel...  \n",
      "\n",
      "Summary statistics of the  dataset:\n",
      "                                                 sentence    emotion  \\\n",
      "count                                              479140     479140   \n",
      "unique                                             479140          6   \n",
      "top     im feeling its time to go their presence is no...  happiness   \n",
      "freq                                                    1     190250   \n",
      "\n",
      "                                                   prompt  \n",
      "count                                              479140  \n",
      "unique                                             479140  \n",
      "top     This text expresses: im feeling its time to go...  \n",
      "freq                                                    1  \n",
      "\n",
      "Information about columns in the dataset:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 479140 entries, 0 to 479139\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count   Dtype \n",
      "---  ------    --------------   ----- \n",
      " 0   sentence  479140 non-null  object\n",
      " 1   emotion   479140 non-null  object\n",
      " 2   prompt    479140 non-null  object\n",
      "dtypes: object(3)\n",
      "memory usage: 11.0+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Specify the path to your CSV file\n",
    "file_path = 'Tasks/emotion_data_merged_4.csv'\n",
    "\n",
    "# Load the dataset\n",
    "dataset = pd.read_csv(file_path)\n",
    "\n",
    "dataset = dataset.dropna(subset=['emotion']).query(\"emotion != 'neutral'\")\n",
    "data = dataset.drop_duplicates()\n",
    "\n",
    "# Shuffle the combined dataset\n",
    "data = data.sample(frac=1).reset_index(drop=True)\n",
    "# Assuming `data` is your DataFrame with 'sentence' and 'emotion' columns\n",
    "data['prompt'] = \"This text expresses: \" + data['sentence'] + \" The emotion is:\"\n",
    "\n",
    "# Display basic information about the combined dataset\n",
    "print(\"Preview of the dataset:\")\n",
    "print(data.head())\n",
    "\n",
    "print(\"\\nSummary statistics of the  dataset:\")\n",
    "print(data.describe())\n",
    "\n",
    "print(\"\\nInformation about columns in the dataset:\")\n",
    "print(data.info())\n",
    "\n",
    "sentences = data['sentence'].values\n",
    "labels = data['emotion'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eefdd250-2fcc-48bb-8533-9d281741c7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the labels\n",
    "label_encoder = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a42bc8f3-0fe3-4751-a063-3d5f61aebaac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-03 13:47:01.607477: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2024-04-03 13:47:01.607834: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14708 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "# Initialize tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "for sent in sentences:\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "        sent,                      \n",
    "        add_special_tokens=True,   \n",
    "        max_length=MAX_LEN,       \n",
    "        padding='max_length',     # Use 'max_length' for explicit padding\n",
    "        truncation=True,           # Explicitly activate truncation\n",
    "        return_attention_mask=True,   \n",
    "        return_tensors='tf',      \n",
    "    )\n",
    "    \n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "    \n",
    "labels = label_encoder.fit_transform(labels)\n",
    "labels = tf.convert_to_tensor(labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4fd3e044-cb31-4e4c-8bcb-e9a892d1b859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure all inputs to train_test_split are numpy arrays\n",
    "input_ids_np = input_ids.numpy() if isinstance(input_ids, tf.Tensor) else input_ids\n",
    "attention_masks_np = attention_masks.numpy() if isinstance(attention_masks, tf.Tensor) else attention_masks\n",
    "labels_np = labels.numpy() if isinstance(labels, tf.Tensor) else labels\n",
    "\n",
    "# Now perform the train-test split\n",
    "X_train, X_val, y_train, y_val = train_test_split(input_ids_np, labels_np, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=labels_np)\n",
    "train_mask, val_mask = train_test_split(attention_masks_np, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=labels_np)\n",
    "\n",
    "X_train = tf.convert_to_tensor(X_train, dtype=tf.int32)\n",
    "X_val = tf.convert_to_tensor(X_val, dtype=tf.int32)\n",
    "y_train = tf.convert_to_tensor(y_train, dtype=tf.int32)\n",
    "y_val = tf.convert_to_tensor(y_val, dtype=tf.int32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e1634f38-7a61-441b-b2f3-744b167270e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TensorFlow datasets for the training and validation sets\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(({\"input_ids\": X_train, \"attention_mask\": train_mask}, y_train)).shuffle(10000).batch(BATCH_SIZE)\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices(({\"input_ids\": X_val, \"attention_mask\": val_mask}, y_val)).batch(BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867f7842-147f-4d19-b4cd-e33146178208",
   "metadata": {},
   "source": [
    "## Model training\n",
    "\n",
    "Setting up the RoBERTa model, defining the training loop, and initiating the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d570dd00-ea60-41e6-8842-260e51f860ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFGPT2Model.\n",
      "\n",
      "All the weights of TFGPT2Model were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2Model for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# Load RoBERTa model\n",
    "model_config = GPT2Config.from_pretrained('gpt2', num_labels=NUM_LABELS)\n",
    "class GPT2ForSequenceClassification(tf.keras.Model):\n",
    "    def __init__(self, model_name, num_labels):\n",
    "        super(GPT2ForSequenceClassification, self).__init__()\n",
    "        self.gpt2 = TFGPT2Model.from_pretrained(model_name)\n",
    "        self.dropout = tf.keras.layers.Dropout(0.1)\n",
    "        self.classifier = tf.keras.layers.Dense(num_labels, activation='softmax')\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        outputs = self.gpt2(inputs)\n",
    "        sequence_output = outputs.last_hidden_state[:, -1, :]\n",
    "        sequence_output = self.dropout(sequence_output)\n",
    "        logits = self.classifier(sequence_output)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "\n",
    "model = GPT2ForSequenceClassification('gpt2', NUM_LABELS)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "\n",
    "# Create TensorFlow datasets\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(({\"input_ids\": input_ids, \"attention_mask\": attention_masks}, labels)).shuffle(10000).batch(BATCH_SIZE)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a9193d4-8db2-4b46-8034-0d403b5f994d",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_accuracy',  # Monitor validation accuracy\n",
    "    patience=3,             # Number of epochs with no improvement after which training will be stopped\n",
    "    verbose=1,              # Log when training is stopped\n",
    "    restore_best_weights=True, # Restore model weights from the epoch with the best value of the monitored quantity\n",
    "    mode='max'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f36a529-e4ed-4115-b45c-b44cc3f9910a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.1,  # Reduction factor for the learning rate\n",
    "    patience=2,  # Number of epochs with no improvement after which learning rate will be reduced\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cd642749-da1b-4160-95c2-8db091c894fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit label encoder and return encoded labels\n",
    "labels_encoded = label_encoder.fit_transform(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "59fae809-f665-41e4-9bcf-850d40a11b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of occurrences of each class\n",
    "class_counts = np.unique(labels_encoded, return_counts=True)[1]\n",
    "\n",
    "# Calculate total number of samples\n",
    "total_samples = len(labels_encoded)\n",
    "\n",
    "# Calculate class weights inversely proportional to the class frequencies\n",
    "class_weights = {i: total_samples/(count * len(class_counts)) for i, count in enumerate(class_counts)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a4875774-e62e-4de9-8f48-8de9746d5d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the directory where you want to save the checkpoints\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "# Ensure the directory exists. If it doesn't, create it.\n",
    "os.makedirs(base_save_path, exist_ok=True)\n",
    "\n",
    "# Specify the checkpoint file path pattern\n",
    "checkpoint_path = os.path.join(base_save_path, \"cp-{epoch:04d}.ckpt\")\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "# Create a ModelCheckpoint callback\n",
    "cp_callback = ModelCheckpoint(\n",
    "    filepath=checkpoint_path, \n",
    "    verbose=1, \n",
    "    save_weights_only=True, \n",
    "    save_best_only=True,  # Saves only the best model\n",
    "    monitor='val_loss',  # Monitoring validation loss to determine the best model\n",
    "    mode='min',  # Since we're monitoring 'val_loss', 'min' mode saves the model when the metric has decreased\n",
    "    save_freq='epoch')  # Saving the model after each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "68d52b99-87b0-4b52-818a-f4e201374fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy', f1_metric, ExactMatchAccuracy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a77f9bbd-4fda-40d1-b49a-b75e8f6eb9c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-03 13:54:12.226826: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14708 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "2024-04-03 13:54:12.402946: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:388] MLIR V1 optimization pass is not enabled\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "in user code:\n\n    File \"/usr/local/lib/python3.11/dist-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"/tmp/ipykernel_426852/3574840235.py\", line 9, in update_state  *\n        y_pred_texts = [extract_label_from_generated_text(text) for text in tf.unstack(y_pred, axis=-1)]\n    File \"/tmp/ipykernel_426852/3505807301.py\", line 5, in extract_label_from_generated_text  *\n        predicted_label_idx_value = sess.run(predicted_label_idx)\n\n    InvalidArgumentError: Graph execution error:\n    \n    Detected at node 'iterator' defined at (most recent call last):\n        File \"<frozen runpy>\", line 198, in _run_module_as_main\n        File \"<frozen runpy>\", line 88, in _run_code\n        File \"/usr/local/lib/python3.11/dist-packages/ipykernel_launcher.py\", line 18, in <module>\n        File \"/usr/local/lib/python3.11/dist-packages/traitlets/config/application.py\", line 1075, in launch_instance\n        File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelapp.py\", line 739, in start\n        File \"/usr/local/lib/python3.11/dist-packages/tornado/platform/asyncio.py\", line 205, in start\n        File \"/usr/lib/python3.11/asyncio/base_events.py\", line 604, in run_forever\n        File \"/usr/lib/python3.11/asyncio/base_events.py\", line 1909, in _run_once\n        File \"/usr/lib/python3.11/asyncio/events.py\", line 80, in _run\n        File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n        File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 534, in process_one\n        File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n        File \"/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py\", line 359, in execute_request\n        File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n        File \"/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py\", line 446, in do_execute\n        File \"/usr/local/lib/python3.11/dist-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n        File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n        File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n        File \"/usr/local/lib/python3.11/dist-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n        File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n        File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n        File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n        File \"/tmp/ipykernel_426852/3662132575.py\", line 2, in <module>\n        File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n        File \"/usr/local/lib/python3.11/dist-packages/keras/src/engine/training.py\", line 1807, in fit\n        File \"/usr/local/lib/python3.11/dist-packages/tensorflow/core/function/polymorphism/function_type.py\", line 356, in placeholder_arguments\n    Node: 'iterator'\n    Detected at node 'gpt2_for_sequence_classification/tfgpt2_model/transformer/h_._8/mlp/c_fc/add/ReadVariableOp/resource' defined at (most recent call last):\n        File \"<frozen runpy>\", line 198, in _run_module_as_main\n        File \"<frozen runpy>\", line 88, in _run_code\n        File \"/usr/local/lib/python3.11/dist-packages/ipykernel_launcher.py\", line 18, in <module>\n        File \"/usr/local/lib/python3.11/dist-packages/traitlets/config/application.py\", line 1075, in launch_instance\n        File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelapp.py\", line 739, in start\n        File \"/usr/local/lib/python3.11/dist-packages/tornado/platform/asyncio.py\", line 205, in start\n        File \"/usr/lib/python3.11/asyncio/base_events.py\", line 604, in run_forever\n        File \"/usr/lib/python3.11/asyncio/base_events.py\", line 1909, in _run_once\n        File \"/usr/lib/python3.11/asyncio/events.py\", line 80, in _run\n        File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n        File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 534, in process_one\n        File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n        File \"/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py\", line 359, in execute_request\n        File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n        File \"/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py\", line 446, in do_execute\n        File \"/usr/local/lib/python3.11/dist-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n        File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n        File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n        File \"/usr/local/lib/python3.11/dist-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n        File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n        File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n        File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n        File \"/tmp/ipykernel_426852/3662132575.py\", line 2, in <module>\n        File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n        File \"/usr/local/lib/python3.11/dist-packages/keras/src/engine/training.py\", line 1807, in fit\n        File \"/usr/local/lib/python3.11/dist-packages/keras/src/engine/training.py\", line 1401, in train_function\n        File \"/usr/local/lib/python3.11/dist-packages/keras/src/engine/training.py\", line 1384, in step_function\n        File \"/usr/local/lib/python3.11/dist-packages/keras/src/engine/training.py\", line 1373, in run_step\n        File \"/usr/local/lib/python3.11/dist-packages/keras/src/engine/training.py\", line 1150, in train_step\n        File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n        File \"/usr/local/lib/python3.11/dist-packages/keras/src/engine/training.py\", line 590, in __call__\n        File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n        File \"/usr/local/lib/python3.11/dist-packages/keras/src/engine/base_layer.py\", line 1149, in __call__\n        File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 96, in error_handler\n        File \"/tmp/ipykernel_426852/3513494651.py\", line 11, in call\n        File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n        File \"/usr/local/lib/python3.11/dist-packages/keras/src/engine/training.py\", line 590, in __call__\n        File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n        File \"/usr/local/lib/python3.11/dist-packages/keras/src/engine/base_layer.py\", line 1149, in __call__\n        File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 96, in error_handler\n        File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_tf_utils.py\", line 783, in run_call_with_unpacked_inputs\n        File \"/usr/local/lib/python3.11/dist-packages/transformers/models/gpt2/modeling_tf_gpt2.py\", line 811, in call\n        File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n        File \"/usr/local/lib/python3.11/dist-packages/keras/src/engine/base_layer.py\", line 1149, in __call__\n        File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 96, in error_handler\n        File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_tf_utils.py\", line 783, in run_call_with_unpacked_inputs\n        File \"/usr/local/lib/python3.11/dist-packages/transformers/models/gpt2/modeling_tf_gpt2.py\", line 518, in call\n        File \"/usr/local/lib/python3.11/dist-packages/transformers/models/gpt2/modeling_tf_gpt2.py\", line 522, in call\n        File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n        File \"/usr/local/lib/python3.11/dist-packages/keras/src/engine/base_layer.py\", line 1149, in __call__\n        File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 96, in error_handler\n        File \"/usr/local/lib/python3.11/dist-packages/transformers/models/gpt2/modeling_tf_gpt2.py\", line 327, in call\n        File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n        File \"/usr/local/lib/python3.11/dist-packages/keras/src/engine/base_layer.py\", line 1149, in __call__\n        File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 96, in error_handler\n        File \"/usr/local/lib/python3.11/dist-packages/transformers/models/gpt2/modeling_tf_gpt2.py\", line 238, in call\n        File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n        File \"/usr/local/lib/python3.11/dist-packages/keras/src/engine/base_layer.py\", line 1149, in __call__\n        File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 96, in error_handler\n        File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_tf_utils.py\", line 3214, in call\n        File \"/usr/local/lib/python3.11/dist-packages/tensorflow/core/function/capture/capture_container.py\", line 141, in capture_by_value\n        File \"/usr/local/lib/python3.11/dist-packages/tensorflow/core/function/capture/capture_container.py\", line 285, in _create_placeholder_helper\n    Node: 'gpt2_for_sequence_classification/tfgpt2_model/transformer/h_._8/mlp/c_fc/add/ReadVariableOp/resource'\n    2 root error(s) found.\n      (0) INVALID_ARGUMENT: You must feed a value for placeholder tensor 'iterator' with dtype resource\n    \t [[{{node iterator}}]]\n      (1) INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gpt2_for_sequence_classification/tfgpt2_model/transformer/h_._8/mlp/c_fc/add/ReadVariableOp/resource' with dtype resource\n    \t [[{{node gpt2_for_sequence_classification/tfgpt2_model/transformer/h_._8/mlp/c_fc/add/ReadVariableOp/resource}}]]\n    0 successful operations.\n    0 derived errors ignored.\n    \n    Original stack trace for 'iterator':\n      File \"<frozen runpy>\", line 198, in _run_module_as_main\n      File \"<frozen runpy>\", line 88, in _run_code\n      File \"/usr/local/lib/python3.11/dist-packages/ipykernel_launcher.py\", line 18, in <module>\n      File \"/usr/local/lib/python3.11/dist-packages/traitlets/config/application.py\", line 1075, in launch_instance\n      File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelapp.py\", line 739, in start\n      File \"/usr/local/lib/python3.11/dist-packages/tornado/platform/asyncio.py\", line 205, in start\n      File \"/usr/lib/python3.11/asyncio/base_events.py\", line 604, in run_forever\n      File \"/usr/lib/python3.11/asyncio/base_events.py\", line 1909, in _run_once\n      File \"/usr/lib/python3.11/asyncio/events.py\", line 80, in _run\n      File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n      File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 534, in process_one\n      File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n      File \"/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py\", line 359, in execute_request\n      File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n      File \"/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py\", line 446, in do_execute\n      File \"/usr/local/lib/python3.11/dist-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n      File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n      File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n      File \"/usr/local/lib/python3.11/dist-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n      File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n      File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n      File \"/tmp/ipykernel_426852/3662132575.py\", line 2, in <module>\n      File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n      File \"/usr/local/lib/python3.11/dist-packages/keras/src/engine/training.py\", line 1807, in fit\n      File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/traceback_utils.py\", line 150, in error_handler\n      File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\", line 832, in __call__\n      File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\", line 888, in _call\n      File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\", line 695, in _initialize\n      File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\", line 178, in trace_function\n      File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\", line 283, in _maybe_define_function\n      File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\", line 303, in _create_concrete_function\n      File \"/usr/local/lib/python3.11/dist-packages/tensorflow/core/function/polymorphism/function_type.py\", line 356, in placeholder_arguments\n      File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/type_spec.py\", line 242, in placeholder_value\n      File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/nest.py\", line 631, in map_structure\n      File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/nest_util.py\", line 1066, in map_structure\n      File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/nest_util.py\", line 1106, in _tf_core_map_structure\n      File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/nest_util.py\", line 1106, in <listcomp>\n      File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/type_spec.py\", line 243, in <lambda>\n      File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/tensor.py\", line 1022, in placeholder_value\n      File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/tensor.py\", line 1060, in _graph_placeholder\n      File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/func_graph.py\", line 670, in _create_op_internal\n      File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py\", line 2652, in _create_op_internal\n      File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py\", line 1160, in from_node_def\n    \n\n\nOriginal stack trace for 'iterator':\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n  File \"<frozen runpy>\", line 88, in _run_code\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel_launcher.py\", line 18, in <module>\n  File \"/usr/local/lib/python3.11/dist-packages/traitlets/config/application.py\", line 1075, in launch_instance\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelapp.py\", line 739, in start\n  File \"/usr/local/lib/python3.11/dist-packages/tornado/platform/asyncio.py\", line 205, in start\n  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 604, in run_forever\n  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 1909, in _run_once\n  File \"/usr/lib/python3.11/asyncio/events.py\", line 80, in _run\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 534, in process_one\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py\", line 359, in execute_request\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py\", line 446, in do_execute\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n  File \"/tmp/ipykernel_426852/3662132575.py\", line 2, in <module>\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/engine/training.py\", line 1807, in fit\n  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/traceback_utils.py\", line 150, in error_handler\n  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\", line 832, in __call__\n  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\", line 888, in _call\n  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\", line 695, in _initialize\n  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\", line 178, in trace_function\n  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\", line 283, in _maybe_define_function\n  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\", line 303, in _create_concrete_function\n  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/core/function/polymorphism/function_type.py\", line 356, in placeholder_arguments\n  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/type_spec.py\", line 242, in placeholder_value\n  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/nest.py\", line 631, in map_structure\n  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/nest_util.py\", line 1066, in map_structure\n  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/nest_util.py\", line 1106, in _tf_core_map_structure\n  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/nest_util.py\", line 1106, in <listcomp>\n  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/type_spec.py\", line 243, in <lambda>\n  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/tensor.py\", line 1022, in placeholder_value\n  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/tensor.py\", line 1060, in _graph_placeholder\n  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/func_graph.py\", line 670, in _create_op_internal\n  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py\", line 2652, in _create_op_internal\n  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py\", line 1160, in from_node_def\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Fit the model with class weights, early stopping, and learning rate scheduler\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr_scheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcp_callback\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_weights\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Use the calculated class weights\u001b[39;49;00m\n\u001b[1;32m      9\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/autograph_util.py:52\u001b[0m, in \u001b[0;36mpy_func_from_autograph.<locals>.autograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[1;32m     51\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mag_error_metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m---> 52\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mag_error_metadata\u001b[38;5;241m.\u001b[39mto_exception(e)\n\u001b[1;32m     53\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.11/dist-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"/tmp/ipykernel_426852/3574840235.py\", line 9, in update_state  *\n        y_pred_texts = [extract_label_from_generated_text(text) for text in tf.unstack(y_pred, axis=-1)]\n    File \"/tmp/ipykernel_426852/3505807301.py\", line 5, in extract_label_from_generated_text  *\n        predicted_label_idx_value = sess.run(predicted_label_idx)\n\n    InvalidArgumentError: Graph execution error:\n    \n    Detected at node 'iterator' defined at (most recent call last):\n        File \"<frozen runpy>\", line 198, in _run_module_as_main\n        File \"<frozen runpy>\", line 88, in _run_code\n        File \"/usr/local/lib/python3.11/dist-packages/ipykernel_launcher.py\", line 18, in <module>\n        File \"/usr/local/lib/python3.11/dist-packages/traitlets/config/application.py\", line 1075, in launch_instance\n        File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelapp.py\", line 739, in start\n        File \"/usr/local/lib/python3.11/dist-packages/tornado/platform/asyncio.py\", line 205, in start\n        File \"/usr/lib/python3.11/asyncio/base_events.py\", line 604, in run_forever\n        File \"/usr/lib/python3.11/asyncio/base_events.py\", line 1909, in _run_once\n        File \"/usr/lib/python3.11/asyncio/events.py\", line 80, in _run\n        File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n        File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 534, in process_one\n        File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n        File \"/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py\", line 359, in execute_request\n        File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n        File \"/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py\", line 446, in do_execute\n        File \"/usr/local/lib/python3.11/dist-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n        File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n        File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n        File \"/usr/local/lib/python3.11/dist-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n        File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n        File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n        File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n        File \"/tmp/ipykernel_426852/3662132575.py\", line 2, in <module>\n        File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n        File \"/usr/local/lib/python3.11/dist-packages/keras/src/engine/training.py\", line 1807, in fit\n        File \"/usr/local/lib/python3.11/dist-packages/tensorflow/core/function/polymorphism/function_type.py\", line 356, in placeholder_arguments\n    Node: 'iterator'\n    Detected at node 'gpt2_for_sequence_classification/tfgpt2_model/transformer/h_._8/mlp/c_fc/add/ReadVariableOp/resource' defined at (most recent call last):\n        File \"<frozen runpy>\", line 198, in _run_module_as_main\n        File \"<frozen runpy>\", line 88, in _run_code\n        File \"/usr/local/lib/python3.11/dist-packages/ipykernel_launcher.py\", line 18, in <module>\n        File \"/usr/local/lib/python3.11/dist-packages/traitlets/config/application.py\", line 1075, in launch_instance\n        File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelapp.py\", line 739, in start\n        File \"/usr/local/lib/python3.11/dist-packages/tornado/platform/asyncio.py\", line 205, in start\n        File \"/usr/lib/python3.11/asyncio/base_events.py\", line 604, in run_forever\n        File \"/usr/lib/python3.11/asyncio/base_events.py\", line 1909, in _run_once\n        File \"/usr/lib/python3.11/asyncio/events.py\", line 80, in _run\n        File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n        File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 534, in process_one\n        File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n        File \"/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py\", line 359, in execute_request\n        File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n        File \"/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py\", line 446, in do_execute\n        File \"/usr/local/lib/python3.11/dist-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n        File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n        File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n        File \"/usr/local/lib/python3.11/dist-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n        File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n        File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n        File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n        File \"/tmp/ipykernel_426852/3662132575.py\", line 2, in <module>\n        File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n        File \"/usr/local/lib/python3.11/dist-packages/keras/src/engine/training.py\", line 1807, in fit\n        File \"/usr/local/lib/python3.11/dist-packages/keras/src/engine/training.py\", line 1401, in train_function\n        File \"/usr/local/lib/python3.11/dist-packages/keras/src/engine/training.py\", line 1384, in step_function\n        File \"/usr/local/lib/python3.11/dist-packages/keras/src/engine/training.py\", line 1373, in run_step\n        File \"/usr/local/lib/python3.11/dist-packages/keras/src/engine/training.py\", line 1150, in train_step\n        File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n        File \"/usr/local/lib/python3.11/dist-packages/keras/src/engine/training.py\", line 590, in __call__\n        File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n        File \"/usr/local/lib/python3.11/dist-packages/keras/src/engine/base_layer.py\", line 1149, in __call__\n        File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 96, in error_handler\n        File \"/tmp/ipykernel_426852/3513494651.py\", line 11, in call\n        File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n        File \"/usr/local/lib/python3.11/dist-packages/keras/src/engine/training.py\", line 590, in __call__\n        File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n        File \"/usr/local/lib/python3.11/dist-packages/keras/src/engine/base_layer.py\", line 1149, in __call__\n        File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 96, in error_handler\n        File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_tf_utils.py\", line 783, in run_call_with_unpacked_inputs\n        File \"/usr/local/lib/python3.11/dist-packages/transformers/models/gpt2/modeling_tf_gpt2.py\", line 811, in call\n        File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n        File \"/usr/local/lib/python3.11/dist-packages/keras/src/engine/base_layer.py\", line 1149, in __call__\n        File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 96, in error_handler\n        File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_tf_utils.py\", line 783, in run_call_with_unpacked_inputs\n        File \"/usr/local/lib/python3.11/dist-packages/transformers/models/gpt2/modeling_tf_gpt2.py\", line 518, in call\n        File \"/usr/local/lib/python3.11/dist-packages/transformers/models/gpt2/modeling_tf_gpt2.py\", line 522, in call\n        File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n        File \"/usr/local/lib/python3.11/dist-packages/keras/src/engine/base_layer.py\", line 1149, in __call__\n        File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 96, in error_handler\n        File \"/usr/local/lib/python3.11/dist-packages/transformers/models/gpt2/modeling_tf_gpt2.py\", line 327, in call\n        File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n        File \"/usr/local/lib/python3.11/dist-packages/keras/src/engine/base_layer.py\", line 1149, in __call__\n        File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 96, in error_handler\n        File \"/usr/local/lib/python3.11/dist-packages/transformers/models/gpt2/modeling_tf_gpt2.py\", line 238, in call\n        File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n        File \"/usr/local/lib/python3.11/dist-packages/keras/src/engine/base_layer.py\", line 1149, in __call__\n        File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 96, in error_handler\n        File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_tf_utils.py\", line 3214, in call\n        File \"/usr/local/lib/python3.11/dist-packages/tensorflow/core/function/capture/capture_container.py\", line 141, in capture_by_value\n        File \"/usr/local/lib/python3.11/dist-packages/tensorflow/core/function/capture/capture_container.py\", line 285, in _create_placeholder_helper\n    Node: 'gpt2_for_sequence_classification/tfgpt2_model/transformer/h_._8/mlp/c_fc/add/ReadVariableOp/resource'\n    2 root error(s) found.\n      (0) INVALID_ARGUMENT: You must feed a value for placeholder tensor 'iterator' with dtype resource\n    \t [[{{node iterator}}]]\n      (1) INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gpt2_for_sequence_classification/tfgpt2_model/transformer/h_._8/mlp/c_fc/add/ReadVariableOp/resource' with dtype resource\n    \t [[{{node gpt2_for_sequence_classification/tfgpt2_model/transformer/h_._8/mlp/c_fc/add/ReadVariableOp/resource}}]]\n    0 successful operations.\n    0 derived errors ignored.\n    \n    Original stack trace for 'iterator':\n      File \"<frozen runpy>\", line 198, in _run_module_as_main\n      File \"<frozen runpy>\", line 88, in _run_code\n      File \"/usr/local/lib/python3.11/dist-packages/ipykernel_launcher.py\", line 18, in <module>\n      File \"/usr/local/lib/python3.11/dist-packages/traitlets/config/application.py\", line 1075, in launch_instance\n      File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelapp.py\", line 739, in start\n      File \"/usr/local/lib/python3.11/dist-packages/tornado/platform/asyncio.py\", line 205, in start\n      File \"/usr/lib/python3.11/asyncio/base_events.py\", line 604, in run_forever\n      File \"/usr/lib/python3.11/asyncio/base_events.py\", line 1909, in _run_once\n      File \"/usr/lib/python3.11/asyncio/events.py\", line 80, in _run\n      File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n      File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 534, in process_one\n      File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n      File \"/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py\", line 359, in execute_request\n      File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n      File \"/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py\", line 446, in do_execute\n      File \"/usr/local/lib/python3.11/dist-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n      File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n      File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n      File \"/usr/local/lib/python3.11/dist-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n      File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n      File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n      File \"/tmp/ipykernel_426852/3662132575.py\", line 2, in <module>\n      File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n      File \"/usr/local/lib/python3.11/dist-packages/keras/src/engine/training.py\", line 1807, in fit\n      File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/traceback_utils.py\", line 150, in error_handler\n      File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\", line 832, in __call__\n      File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\", line 888, in _call\n      File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\", line 695, in _initialize\n      File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\", line 178, in trace_function\n      File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\", line 283, in _maybe_define_function\n      File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\", line 303, in _create_concrete_function\n      File \"/usr/local/lib/python3.11/dist-packages/tensorflow/core/function/polymorphism/function_type.py\", line 356, in placeholder_arguments\n      File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/type_spec.py\", line 242, in placeholder_value\n      File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/nest.py\", line 631, in map_structure\n      File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/nest_util.py\", line 1066, in map_structure\n      File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/nest_util.py\", line 1106, in _tf_core_map_structure\n      File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/nest_util.py\", line 1106, in <listcomp>\n      File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/type_spec.py\", line 243, in <lambda>\n      File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/tensor.py\", line 1022, in placeholder_value\n      File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/tensor.py\", line 1060, in _graph_placeholder\n      File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/func_graph.py\", line 670, in _create_op_internal\n      File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py\", line 2652, in _create_op_internal\n      File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py\", line 1160, in from_node_def\n    \n\n\nOriginal stack trace for 'iterator':\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n  File \"<frozen runpy>\", line 88, in _run_code\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel_launcher.py\", line 18, in <module>\n  File \"/usr/local/lib/python3.11/dist-packages/traitlets/config/application.py\", line 1075, in launch_instance\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelapp.py\", line 739, in start\n  File \"/usr/local/lib/python3.11/dist-packages/tornado/platform/asyncio.py\", line 205, in start\n  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 604, in run_forever\n  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 1909, in _run_once\n  File \"/usr/lib/python3.11/asyncio/events.py\", line 80, in _run\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 534, in process_one\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py\", line 359, in execute_request\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py\", line 446, in do_execute\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n  File \"/tmp/ipykernel_426852/3662132575.py\", line 2, in <module>\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/engine/training.py\", line 1807, in fit\n  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/traceback_utils.py\", line 150, in error_handler\n  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\", line 832, in __call__\n  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\", line 888, in _call\n  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\", line 695, in _initialize\n  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\", line 178, in trace_function\n  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\", line 283, in _maybe_define_function\n  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\", line 303, in _create_concrete_function\n  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/core/function/polymorphism/function_type.py\", line 356, in placeholder_arguments\n  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/type_spec.py\", line 242, in placeholder_value\n  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/nest.py\", line 631, in map_structure\n  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/nest_util.py\", line 1066, in map_structure\n  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/nest_util.py\", line 1106, in _tf_core_map_structure\n  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/nest_util.py\", line 1106, in <listcomp>\n  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/type_spec.py\", line 243, in <lambda>\n  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/tensor.py\", line 1022, in placeholder_value\n  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/tensor.py\", line 1060, in _graph_placeholder\n  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/func_graph.py\", line 670, in _create_op_internal\n  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py\", line 2652, in _create_op_internal\n  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py\", line 1160, in from_node_def\n"
     ]
    }
   ],
   "source": [
    "# Fit the model with class weights, early stopping, and learning rate scheduler\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=val_dataset,\n",
    "    callbacks=[early_stopping, lr_scheduler, cp_callback],\n",
    "    class_weight=class_weights  # Use the calculated class weights\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f1b6e5-e6e1-4ddb-bb51-01b727d79713",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.evaluate(val_dataset)\n",
    "print(f\"Validation loss: {results[0]}, Validation accuracy: {results[1]}, Validation F1 Score: {results[2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88aa3bad-ff0e-4cd0-8d2f-97681e658cf7",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "Generate usefull insights on the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cf99fa-62ee-4208-b3d4-415308ac8a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history):\n",
    "    fig, ax = plt.subplots(1, 3, figsize=(20, 5))\n",
    "    \n",
    "    # Plot training & validation accuracy values\n",
    "    ax[0].plot(history.history['accuracy'])\n",
    "    ax[0].plot(history.history['val_accuracy'])\n",
    "    ax[0].set_title('Model accuracy')\n",
    "    ax[0].set_ylabel('Accuracy')\n",
    "    ax[0].set_xlabel('Epoch')\n",
    "    ax[0].legend(['Train', 'Val'], loc='upper left')\n",
    "\n",
    "    # Plot training & validation loss values\n",
    "    ax[1].plot(history.history['loss'])\n",
    "    ax[1].plot(history.history['val_loss'])\n",
    "    ax[1].set_title('Model loss')\n",
    "    ax[1].set_ylabel('Loss')\n",
    "    ax[1].set_xlabel('Epoch')\n",
    "    ax[1].legend(['Train', 'Val'], loc='upper left')\n",
    "    \n",
    "    # Plot training & validation F1 score values\n",
    "    ax[2].plot(history.history['f1_metric'])\n",
    "    ax[2].plot(history.history['val_f1_metric'])\n",
    "    ax[2].set_title('Model F1 Score')\n",
    "    ax[2].set_ylabel('F1 Score')\n",
    "    ax[2].set_xlabel('Epoch')\n",
    "    ax[2].legend(['Train', 'Val'], loc='upper left')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "plot_training_history(history)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d46d917-0eca-42a8-b5bc-5389d7250f70",
   "metadata": {},
   "source": [
    "##  Generate the Confusion Matrix and Metrics\n",
    "\n",
    "With the true labels and predictions, we can now generate a confusion matrix and calculate other evaluation metrics like precision, recall, and F1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68483e9-474e-44ae-9a33-3202ece3125a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Assume `predicted_labels` are extracted and transformed to match `y_val`'s encoding\n",
    "# Generate the confusion matrix\n",
    "cm = confusion_matrix(y_val, predicted_labels)\n",
    "\n",
    "# Plotting\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', ax=ax, cmap='Blues',\n",
    "            xticklabels=label_encoder.classes_,\n",
    "            yticklabels=label_encoder.classes_)\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95905cbc-5b2e-4c2e-a361-fde96dabbac5",
   "metadata": {},
   "source": [
    "## Prepare the Submission DataFrame and Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8001b0c2-e8a2-4f15-84a4-8c5fd17a6bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming `test_df` is your test DataFrame and it contains a 'sentence' column\n",
    "test_prompts = \"This text expresses: \" + test_df['sentence'] + \" The emotion is:\"\n",
    "\n",
    "# Generate predictions\n",
    "predicted_texts = []  # Placeholder for generated texts\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    inputs = tokenizer(prompt, return_tensors='tf', padding=True, truncation=True, max_length=MAX_LEN)\n",
    "    outputs = model.generate(**inputs, max_length=MAX_LEN + 10)  # Adjust max_length if necessary\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    predicted_label = extract_label_from_generated_text(generated_text)  # Use the previously discussed extraction function\n",
    "    predicted_texts.append(predicted_label)\n",
    "\n",
    "# Assuming you can map predicted_texts to the original labels (e.g., through a dictionary or direct matching)\n",
    "predicted_labels = [text_to_label_mapping[text] for text in predicted_texts]  # Implement this mapping\n",
    "\n",
    "# Prepare the submission DataFrame\n",
    "submission_df = pd.DataFrame({\n",
    "    'id': test_df['id'],  # Assuming there's an 'id' column\n",
    "    'emotion': predicted_labels\n",
    "})\n",
    "\n",
    "# Save the submission file\n",
    "submission_df.to_csv('GPT-2_v1_submission.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2daffd60-882e-48d9-8501-7d35f341f851",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path = \"GPT-2_v1\"\n",
    "tokenizer_save_path = \"GPT-2_v1\"\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained(model_save_path)\n",
    "\n",
    "# Save the tokenizer\n",
    "tokenizer.save_pretrained(tokenizer_save_path)\n",
    "import joblib\n",
    "\n",
    "# Save the label encoder or any mapping used for label-to-text conversions\n",
    "joblib.dump(text_to_label_mapping, 'GPT-2_v1.joblib')  # Adjust as necessary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f5a1c8-fe06-4b18-b0b3-7f8008df9a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sub = 'Roberta_V3_1_task12_2.csv'\n",
    "\n",
    "# Load the dataset\n",
    "data_submission = pd.read_csv(data_sub)\n",
    "\n",
    "# Define the mapping from integer labels to emotion names\n",
    "emotion_mapping = {\n",
    "    0: 'anger',\n",
    "    1: 'disgust',\n",
    "    2: 'fear',\n",
    "    3: 'happiness',\n",
    "    4: 'sadness',\n",
    "    5: 'surprise'\n",
    "}\n",
    "\n",
    "# Apply the mapping to the 'emotion' column\n",
    "data_submission['emotion'] = data_submission['emotion'].map(emotion_mapping)\n",
    "# Save the submission file\n",
    "data_submission.to_csv('GPT-2_v1.csv', index=False, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7602b931-c659-4489-82b3-365c26ae6be9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2891a1f8-b454-43f0-8bed-86d08f849645",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
