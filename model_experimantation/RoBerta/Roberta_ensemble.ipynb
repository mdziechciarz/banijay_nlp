{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5392aec5-64ec-4c6b-84c4-856f9844e26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c8fc6cb-1b24-4193-84c5-2f5b9a8be2c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-29 17:42:15.949648: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-29 17:42:15.949702: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-29 17:42:15.951161: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-29 17:42:15.957770: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModelForSequenceClassification, AutoTokenizer\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9488306b-01eb-41b2-8606-fc0a0aee8475",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-29 17:42:19.946010: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2024-03-29 17:42:19.946343: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21222 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at Roberta_V3_task12/model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at Roberta_V3_1_task12/model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at Roberta_V3_1p_task12/model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at Roberta_V7_task12/model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# Assuming all models have been correctly saved with their configurations and tokenizer\n",
    "# For RoBERTa V3 and V4 (based on `roberta-small`)\n",
    "tokenizer_small = AutoTokenizer.from_pretrained('Roberta_V3_1_task12/tokenizer')\n",
    "\n",
    "# For RoBERTa V6 (based on `roberta-large`)\n",
    "tokenizer_large = AutoTokenizer.from_pretrained('Roberta_V7_task12/tokenizer')\n",
    "\n",
    "# Load models\n",
    "model_v2 = TFAutoModelForSequenceClassification.from_pretrained('Roberta_V3_task12/model')\n",
    "model_v3 = TFAutoModelForSequenceClassification.from_pretrained('Roberta_V3_1_task12/model')\n",
    "model_v4 = TFAutoModelForSequenceClassification.from_pretrained('Roberta_V3_1p_task12/model')\n",
    "model_v6 = TFAutoModelForSequenceClassification.from_pretrained('Roberta_V7_task12/model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b6728f0-6cb2-42d2-921e-8340a996628c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expanded contraction mapping\n",
    "contractions_dict = {\n",
    "    \"i'm\": \"i am\", \"im\": \"i am\", \"i m\": \"i am\",\n",
    "    \"you're\": \"you are\", \"youre\": \"you are\",\n",
    "    \"he's\": \"he is\", \"hes\": \"he is\",\n",
    "    \"she's\": \"she is\", \"shes\": \"she is\",\n",
    "    \"it's\": \"it is\", \"its\": \"it is\",\n",
    "    \"we're\": \"we are\", \"were\": \"we are\",\n",
    "    \"they're\": \"they are\", \"theyre\": \"they are\",\n",
    "    \"that's\": \"that is\", \"thats\": \"that is\", \"that s\": \"that is\",\n",
    "    \"that'd\": \"that would\", \"thatd\": \"that would\",\n",
    "    \"who's\": \"who is\", \"whos\": \"who is\",\n",
    "    \"what's\": \"what is\", \"whats\": \"what is\",\n",
    "    \"where's\": \"where is\", \"wheres\": \"where is\",\n",
    "    \"when's\": \"when is\", \"whens\": \"when is\",\n",
    "    \"why's\": \"why is\", \"whys\": \"why is\",\n",
    "    \"how's\": \"how is\", \"hows\": \"how is\",\n",
    "    \"ain't\": \"am not\", \"aint\": \"am not\",\n",
    "    \"aren't\": \"are not\", \"arent\": \"are not\",\n",
    "    \"isn't\": \"is not\", \"isnt\": \"is not\",\n",
    "    \"wasn't\": \"was not\", \"wasnt\": \"was not\",\n",
    "    \"weren't\": \"were not\", \"werent\": \"were not\",\n",
    "    \"haven't\": \"have not\", \"havent\": \"have not\",\n",
    "    \"hasn't\": \"has not\", \"hasnt\": \"has not\",\n",
    "    \"hadn't\": \"had not\", \"hadnt\": \"had not\",\n",
    "    \"won't\": \"will not\", \"wont\": \"will not\",\n",
    "    \"wouldn't\": \"would not\", \"wouldnt\": \"would not\",\n",
    "    \"don't\": \"do not\", \"dont\": \"do not\",\n",
    "    \"doesn't\": \"does not\", \"doesnt\": \"does not\",\n",
    "    \"didn't\": \"did not\", \"didnt\": \"did not\",\n",
    "    \"can't\": \"cannot\", \"cant\": \"cannot\",\n",
    "    \"couldn't\": \"could not\", \"couldnt\": \"could not\",\n",
    "    \"shouldn't\": \"should not\", \"shouldnt\": \"should not\",\n",
    "    \"mightn't\": \"might not\", \"mightnt\": \"might not\",\n",
    "    \"mustn't\": \"must not\", \"mustnt\": \"must not\",\n",
    "}\n",
    "\n",
    "# Create a regex pattern for contractions, enabling case-insensitive matching\n",
    "contraction_patterns = r'\\b(' + '|'.join([re.escape(k) for k in contractions_dict.keys()]) + r')\\b'\n",
    "contractions_re = re.compile(contraction_patterns, re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9666256d-d245-4bf2-9a2f-9f93ef2cd955",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_contractions(text, contractions_dict=contractions_dict):\n",
    "    def replace(match):\n",
    "        # Fetch the correct replacement string from the dictionary using lowercase match\n",
    "        return contractions_dict[match.group().lower()]\n",
    "    # Use the sub method to replace all matches\n",
    "    expanded_text = contractions_re.sub(replace, text)\n",
    "    return expanded_text\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.strip()  # Strip leading/trailing whitespace\n",
    "    text = expand_contractions(text)  # Expand contractions\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)  # Remove URLs\n",
    "    text = re.sub(r'[\\U00010000-\\U0010ffff]', '', text)  # Remove emoticons and unicode characters\n",
    "    \n",
    "    # Split text into sentences, strip each sentence, then join back with a space\n",
    "    sentences = sent_tokenize(text)\n",
    "    cleaned_sentences = [sentence.strip() for sentence in sentences]\n",
    "    text = ' '.join(cleaned_sentences)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c8ba63a-5765-4423-93ae-e3e35c751f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the test dataset\n",
    "test_df = pd.read_csv('test (1).csv', sep='\\t')\n",
    "test_df['sentence'] = test_df['sentence'].apply(clean_text)\n",
    "def prepare_data(tokenizer, sentences, max_length=128):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    for sent in sentences:\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "            sent,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_length,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',  # PyTorch tensors\n",
    "        )\n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "    # Convert lists into tensors\n",
    "    input_ids = tf.concat(input_ids, axis=0)\n",
    "    attention_masks = tf.concat(attention_masks, axis=0)\n",
    "    return input_ids, attention_masks\n",
    "\n",
    "# Prepare data with both tokenizers\n",
    "test_input_ids_small, test_attention_masks_small = prepare_data(tokenizer_small, test_df['sentence'].values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e98be293-acf9-4758-b518-805fab37fdf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(logits):\n",
    "    if logits.ndim == 1:\n",
    "        # This means we have only one set of logits, we expand the dimension to apply softmax\n",
    "        logits = np.expand_dims(logits, 0)\n",
    "    e_logits = np.exp(logits - np.max(logits, axis=-1, keepdims=True))\n",
    "    return e_logits / e_logits.sum(axis=-1, keepdims=True)\n",
    "\n",
    "def get_average_confidence_score(predictions):\n",
    "    # First, ensure predictions is a 2D array\n",
    "    if predictions.ndim == 1:\n",
    "        predictions = np.expand_dims(predictions, 0)\n",
    "        \n",
    "    probabilities = softmax(predictions)\n",
    "    average_confidence_per_class = np.mean(probabilities, axis=0)\n",
    "    return average_confidence_per_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72d1cc9-0579-43b3-a791-a40670854bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45/45 [==============================] - 12s 86ms/step\n",
      "45/45 [==============================] - 7s 86ms/step\n",
      "45/45 [==============================] - 8s 88ms/step\n"
     ]
    }
   ],
   "source": [
    "def get_predictions_tf(model, input_ids, attention_masks):\n",
    "    # Use the model to predict the outcomes\n",
    "    outputs = model.predict([input_ids, attention_masks])\n",
    "    # Access the logits from the structured output\n",
    "    logits = outputs.logits\n",
    "    # Use np.argmax to get the most likely class label\n",
    "    predicted_labels = np.argmax(logits, axis=1)\n",
    "    return predicted_labels\n",
    "predictions_v2 = get_predictions_tf(model_v2, test_input_ids_small, test_attention_masks_small)\n",
    "predictions_v3 = get_predictions_tf(model_v3, test_input_ids_small, test_attention_masks_small)\n",
    "predictions_v4 = get_predictions_tf(model_v4, test_input_ids_small, test_attention_masks_small)\n",
    "predictions_v6 = get_predictions_tf(model_v6, test_input_ids_small, test_attention_masks_small)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302f555b-6f7c-4134-a33e-2d9b122a0477",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reassign_classes_with_count(predictions_stack, threshold_ratio):\n",
    "    happiness_index = 3  # index of 'happiness' in emotion_mapping\n",
    "    surprise_index = 5  # index of 'surprise' in emotion_mapping\n",
    "    change_count = 0  # Initialize the count of changed sentences\n",
    "    \n",
    "    # Get the indices of the maximum confidence scores along axis=0\n",
    "    max_confidence_indices = np.argmax(predictions_stack, axis=0)\n",
    "    \n",
    "    for i in range(max_confidence_indices.shape[1]):  # Iterate over each prediction\n",
    "        if max_confidence_indices[0, i] == happiness_index:\n",
    "            happiness_score = predictions_stack[happiness_index, i]\n",
    "            surprise_score = predictions_stack[surprise_index, i]\n",
    "            if happiness_score * threshold_ratio >= surprise_score:\n",
    "                max_confidence_indices[0, i] = surprise_index  # Reassign to 'surprise'\n",
    "                change_count += 1  # Increment the count\n",
    "    \n",
    "    return max_confidence_indices, change_count\n",
    "\n",
    "# Define the threshold ratio; for example, surprise should be no more than 20% less confident than happiness\n",
    "threshold_ratio = 0.8  # 20% below happiness\n",
    "\n",
    "# Stack predictions for voting\n",
    "predictions_stack = np.vstack([predictions_v3, predictions_v4, predictions_v6])\n",
    "\n",
    "# Apply the reassigning logic and get the count of changes\n",
    "final_predictions, sentences_changed = reassign_classes_with_count(predictions_stack, threshold_ratio)\n",
    "\n",
    "# Convert numeric predictions to emotion labels\n",
    "final_predictions_labels = [emotion_mapping[pred] for pred in final_predictions.flatten()]\n",
    "\n",
    "# Print the final predictions and the amount of sentences changed\n",
    "print(final_predictions_labels)\n",
    "print(f\"Number of sentences reassigned from 'happiness' to 'surprise': {sentences_changed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad3572e-dabe-49f9-a605-c084131b568d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Load the LabelEncoder for V6 since you are using predictions based on the large model\n",
    "label_encoder_path_v6 = 'Roberta_V3_1_task12/label_encoder.joblib'\n",
    "label_encoder = joblib.load(label_encoder_path_v6)\n",
    "\n",
    "# Convert numeric predictions back to original labels using the LabelEncoder\n",
    "predicted_emotions = label_encoder.inverse_transform(final_predictions)\n",
    "\n",
    "# Prepare the submission DataFrame\n",
    "submission_df = pd.DataFrame({\n",
    "    'id': test_df['id'],  # Assuming there's an 'id' column in your test dataset\n",
    "    'emotion': predicted_emotions\n",
    "})\n",
    "\n",
    "# Save the submission file\n",
    "submission_df.to_csv('Roberta_ensemble_v3.csv', index=False, sep=',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db01bef-9e62-4125-849c-15cdb1a82deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sub = 'Roberta_ensemble_v3.csv'\n",
    "\n",
    "# Load the dataset\n",
    "data_submission = pd.read_csv(data_sub)\n",
    "\n",
    "# Define the mapping from integer labels to emotion names\n",
    "emotion_mapping = {\n",
    "    0: 'anger',\n",
    "    1: 'disgust',\n",
    "    2: 'fear',\n",
    "    3: 'happiness',\n",
    "    4: 'sadness',\n",
    "    5: 'surprise'\n",
    "}\n",
    "\n",
    "# Apply the mapping to the 'emotion' column\n",
    "data_submission['emotion'] = data_submission['emotion'].map(emotion_mapping)\n",
    "# Save the submission file\n",
    "data_submission.to_csv('Roberta_ensemble_v3.csv', index=False, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d6939d-1c24-4630-b167-20e7d8348166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the average confidence score per class for each model\n",
    "average_confidence_v2 = get_average_confidence_score(predictions_v2)\n",
    "average_confidence_v3 = get_average_confidence_score(predictions_v3)\n",
    "average_confidence_v4 = get_average_confidence_score(predictions_v4)\n",
    "average_confidence_v6 = get_average_confidence_score(predictions_v6)\n",
    "\n",
    "# Now you have the average confidence per class for each model\n",
    "print(\"Average confidence per class for model_v2:\", average_confidence_v2)\n",
    "print(\"Average confidence per class for model_v3:\", average_confidence_v3)\n",
    "print(\"Average confidence per class for model_v4:\", average_confidence_v4)\n",
    "print(\"Average confidence per class for model_v6:\", average_confidence_v6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f5a3d5-7de2-4b8f-8a13-4f1bac92c2af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
