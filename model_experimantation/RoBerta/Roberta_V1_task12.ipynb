{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3860487-659f-4ba5-95ec-64072410d2a8",
   "metadata": {},
   "source": [
    "## Importing important libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d60ff81-1bf9-4d08-9789-1dfa121c3268",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, AdamW\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531bfce8-f56d-42a6-830f-27adff87b0e6",
   "metadata": {},
   "source": [
    "## Hyperparameters \n",
    "\n",
    "Easy and straightforward hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75c067a8-3c55-4677-9bae-9fe0b9738f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 1e-5\n",
    "EPOCHS = 3\n",
    "TEST_SIZE = 0.2\n",
    "RANDOM_STATE = 42\n",
    "MAX_LEN = 128  # Maximum sequence length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856fb3dc-2580-4382-b82a-59e7face147a",
   "metadata": {},
   "source": [
    "## Setup and Data Preparation\n",
    "\n",
    "Loading the dataset, preprocessing the text, and preparing the data for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e22f5db7-7207-422e-9c13-c5a4dd39415e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preview of the combined dataset:\n",
      "                                            sentence    emotion\n",
      "0  To everyone's delight, Emma smiled a burst of ...  happiness\n",
      "1      i feel violent and i want you to recognize it      anger\n",
      "2  i admit i feel a lil pained when i realized w ...    sadness\n",
      "3  i feel a little envious of moya o grady and ma...      anger\n",
      "4  i browsed through the stitches that my janome ...  happiness\n",
      "\n",
      "Summary statistics of the combined dataset:\n",
      "       sentence    emotion\n",
      "count    575106     575106\n",
      "unique   562924          6\n",
      "top        when  happiness\n",
      "freq          4     203489\n",
      "\n",
      "Information about columns in the combined dataset:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 575106 entries, 0 to 575105\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count   Dtype \n",
      "---  ------    --------------   ----- \n",
      " 0   sentence  575106 non-null  object\n",
      " 1   emotion   575106 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 8.8+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Load the datasets\n",
    "dataset1_file = 'dataset_stijn_generated.csv'\n",
    "dataset2_file = 'cleaned_combined_dataset.csv'\n",
    "dataset3_file = 'emotion_data_merged_4.csv'\n",
    "try:\n",
    "    dataset1 = pd.read_csv(dataset1_file)\n",
    "    dataset2 = pd.read_csv(dataset2_file)\n",
    "    dataset3 = pd.read_csv(dataset3_file)\n",
    "except FileNotFoundError:\n",
    "    print(\"One or both of the files not found.\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"Error reading file(s): {e}\")\n",
    "    exit()\n",
    "\n",
    "# Concatenate datasets vertically\n",
    "data = pd.concat([dataset1, dataset2, dataset3], ignore_index=True)\n",
    "\n",
    "data_combined = data.dropna(subset=['emotion']).query(\"emotion != 'neutral'\")\n",
    "data = data_combined.drop_duplicates()\n",
    "\n",
    "# Shuffle the combined dataset\n",
    "data = data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Display basic information about the combined dataset\n",
    "print(\"Preview of the combined dataset:\")\n",
    "print(data.head())\n",
    "\n",
    "print(\"\\nSummary statistics of the combined dataset:\")\n",
    "print(data.describe())\n",
    "\n",
    "print(\"\\nInformation about columns in the combined dataset:\")\n",
    "print(data.info())\n",
    "\n",
    "sentences = data['sentence'].values\n",
    "labels = data['emotion'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5739e736-f088-43ec-8357-3a1f569235d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function expects numpy arrays or lists for y_true and y_pred\n",
    "def compute_f1(y_true, y_pred):\n",
    "    # Since y_pred would be logits or probabilities from the model, \n",
    "    # we need to convert these to discrete predictions.\n",
    "    # Assuming y_pred is already the discrete predictions after using torch.argmax on logits\n",
    "    return f1_score(y_true, y_pred, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "060c275e-e466-42b3-a33d-d0ee490c4d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "labels = label_encoder.fit_transform(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf2759e2-324b-43a0-9956-188eb1c91285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization and Data Preparation\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "input_ids = []\n",
    "attention_masks = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a42bc8f3-0fe3-4751-a063-3d5f61aebaac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:2688: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "for sent in sentences:\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = MAX_LEN,      # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attention masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "    \n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    attention_masks.append(encoded_dict['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81a74dd8-af09-4d29-a303-3a02dd587867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert lists into tensors\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f621d321-6c47-49fc-a1a3-38c5ab496b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and validation sets\n",
    "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "train_size = int((1 - TEST_SIZE) * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "validation_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867f7842-147f-4d19-b4cd-e33146178208",
   "metadata": {},
   "source": [
    "## Model training\n",
    "\n",
    "Setting up the RoBERTa model, defining the training loop, and initiating the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d570dd00-ea60-41e6-8842-260e51f860ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Model Setup\n",
    "model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=len(label_encoder.classes_))\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f1b6e5-e6e1-4ddb-bb51-01b727d79713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 3 ========\n"
     ]
    }
   ],
   "source": [
    "# Use GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Training Loop\n",
    "for epoch_i in range(0, EPOCHS):\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, EPOCHS))\n",
    "    model.train()\n",
    "    \n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        \n",
    "        model.zero_grad()        \n",
    "        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(\"Training loss: {0:.2f}\".format(loss.item()))\n",
    "\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88aa3bad-ff0e-4cd0-8d2f-97681e658cf7",
   "metadata": {},
   "source": [
    "## Load and Preprocess the Test Set\n",
    "\n",
    "Load the test set, preprocess it similarly to the training and validation datasets, ensuring the same tokenizer and sequence length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cf99fa-62ee-4208-b3d4-415308ac8a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionDataset(Dataset):\n",
    "    def __init__(self, tokenizer, sentences, labels=None, max_len=128):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.sentences = sentences\n",
    "        self.labels = labels\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        sentence = str(self.sentences[item])\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "          sentence,\n",
    "          add_special_tokens=True,\n",
    "          max_length=self.max_len,\n",
    "          return_token_type_ids=False,\n",
    "          pad_to_max_length=True,\n",
    "          return_attention_mask=True,\n",
    "          return_tensors='pt',\n",
    "        )\n",
    "        \n",
    "        inputs = {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten()\n",
    "        }\n",
    "        \n",
    "        if self.labels is not None:\n",
    "            label = self.labels[item]\n",
    "            return inputs, label\n",
    "        \n",
    "        return inputs\n",
    "\n",
    "# Load your test set\n",
    "df_test = pd.read_csv('test_stijn.csv')\n",
    "\n",
    "# Preprocess labels\n",
    "label_encoder = LabelEncoder()\n",
    "labels = label_encoder.fit_transform(df_test['emotion'].values)\n",
    "sentences = df_test['sentence'].values\n",
    "\n",
    "test_dataset = EmotionDataset(tokenizer, sentences, labels, max_len=MAX_LEN)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9c8266-c948-4017-8b3e-b09ed4d010f7",
   "metadata": {},
   "source": [
    "## Predict Emotions on the Test Set\n",
    "Predict the emotions for the test set sentences using the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35f1060-0977-4746-b5a5-c32ab244a1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader, device):\n",
    "    model = model.eval()\n",
    "\n",
    "    true_labels = []\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in data_loader:\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(**inputs)\n",
    "            _, preds = torch.max(outputs.logits, dim=1)\n",
    "\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "\n",
    "    return true_labels, predictions\n",
    "\n",
    "true_labels, predictions = evaluate(model, test_loader, device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d46d917-0eca-42a8-b5bc-5389d7250f70",
   "metadata": {},
   "source": [
    "##  Generate the Confusion Matrix and Metrics\n",
    "\n",
    "With the true labels and predictions, we can now generate a confusion matrix and calculate other evaluation metrics like precision, recall, and F1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68483e9-474e-44ae-9a33-3202ece3125a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "cm = confusion_matrix(true_labels, predictions)\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "sns.heatmap(cm_normalized, annot=True, ax=ax, cmap='Blues', fmt='.2%')\n",
    "ax.set_xlabel('Predicted labels')\n",
    "ax.set_ylabel('True labels')\n",
    "ax.set_title('Confusion Matrix')\n",
    "ax.xaxis.set_ticklabels(label_encoder.classes_, rotation=45)\n",
    "ax.yaxis.set_ticklabels(label_encoder.classes_, rotation=45)\n",
    "plt.show()\n",
    "\n",
    "# Classification Report\n",
    "print(classification_report(true_labels, predictions, target_names=label_encoder.classes_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376d9151-b724-4c19-a3be-bb376e7e0ef6",
   "metadata": {},
   "source": [
    "## Preprocessing and Data Loading for PyTorch\n",
    "\n",
    "Loading and preprocessing the test set for kaggle competition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc0cb33-0bb1-4794-ae17-7bf6669303ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming `tokenizer` is your pre-trained tokenizer (e.g., RobertaTokenizer)\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, sentences, tokenizer, max_len):\n",
    "        self.sentences = sentences\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        sentence = str(self.sentences[item])\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            sentence,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "            truncation=True\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten()\n",
    "        }\n",
    "\n",
    "# Load the test dataset\n",
    "test_df = pd.read_csv('test (1).csv', sep='\\t')\n",
    "\n",
    "# Remove punctuation from the 'sentence' column\n",
    "test_df['sentence'] = test_df['sentence'].str.replace(r'[^\\w\\s]', '', regex=True)\n",
    "\n",
    "# Assume MAX_LEN and tokenizer are defined\n",
    "test_dataset = TestDataset(test_df['sentence'].tolist(), tokenizer, MAX_LEN)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128)  # Match the batch size or adjust based on your GPU\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0639a8f-d9ed-4b14-ae0c-3f4fdb524093",
   "metadata": {},
   "source": [
    "## Make Predictions with the PyTorch Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4de7d5-916b-4ada-957e-64312a112ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()  # Set the model to evaluation mode\n",
    "predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        input_ids = data['input_ids'].to(device)\n",
    "        attention_mask = data['attention_mask'].to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        preds = torch.argmax(outputs.logits, dim=1)\n",
    "        predictions.extend(preds.cpu().numpy())\n",
    "\n",
    "# Assuming `le` is your label encoder\n",
    "predicted_emotions = le.inverse_transform(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95905cbc-5b2e-4c2e-a361-fde96dabbac5",
   "metadata": {},
   "source": [
    "## Prepare the Submission DataFrame and Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ad251a-c14d-4278-81e5-1115b664269a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the submission DataFrame\n",
    "submission_df = pd.DataFrame({\n",
    "    'id': test_df.index,  # Use original test DataFrame index as ID\n",
    "    'emotion': predicted_emotions\n",
    "})\n",
    "\n",
    "# Save the submission file\n",
    "submission_df.to_csv('submission_torch.csv', index=False, sep=',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee01ecc1-9198-4279-a8ea-9e9c60269985",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
